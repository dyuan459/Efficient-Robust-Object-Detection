{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://github.com/miladlink/TinyYoloV2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "id": "WVDh0TibRADG"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from PIL import Image\n",
    "from utils.YOLOv2 import *\n",
    "from pycocotools.coco import COCO\n",
    "from models.YOLOv3 import load_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "id": "jlQVknSfeKt4"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=0.70s)\n",
      "creating index...\n",
      "index created!\n"
     ]
    }
   ],
   "source": [
    "modelv = 3\n",
    "\n",
    "if modelv == 2:\n",
    "    model = load_model_v2(weights = './weights/yolov2-tiny-voc.weights').to(device)\n",
    "    class_names = ['aeroplane', 'bicycle', 'bird', 'boat', 'bottle', 'bus', 'car', 'cat', 'chair', 'cow', 'diningtable', 'dog', 'horse', 'motorbike', 'person', 'pottedplant', 'sheep', 'sofa', 'train', 'TVmonitor'] \n",
    "    root = \"./data/VOC2007/JPEGImages\"\n",
    "    annFile = \"./data/VOC2007/annotations/val.json\"\n",
    "    coco_gld = COCO(annFile) # pascal\n",
    "    \n",
    "elif modelv == 3:\n",
    "    model = load_model(\"./config/yolov3.cfg\", \"./weights/yolov3.weights\")\n",
    "    class_names = ['person', 'bicycle', 'car', 'motorbike', 'aeroplane', 'bus', 'train', 'truck', 'boat', 'traffic light', 'fire hydrant', 'stop sign', 'parking meter', 'bench', 'bird', 'cat', 'dog', 'horse', 'sheep', 'cow', 'elephant', 'bear', 'zebra', 'giraffe', 'backpack', 'umbrella', 'handbag', 'tie', 'suitcase', 'frisbee', 'skis', 'snowboard', 'sports ball', 'kite', 'baseball bat', 'baseball glove', 'skateboard', 'surfboard', 'tennis racket', 'bottle', 'wine glass', 'cup', 'fork', 'knife', 'spoon', 'bowl', 'banana', 'apple', 'sandwich', 'orange', 'broccoli', 'carrot', 'hot dog', 'pizza', 'donut', 'cake', 'chair', 'sofa', 'pottedplant', 'bed', 'diningtable', 'toilet', 'tvmonitor', 'laptop', 'mouse', 'remote', 'keyboard', 'cell phone', 'microwave', 'oven', 'toaster', 'sink', 'refrigerator', 'book', 'clock', 'vase', 'scissors', 'teddy bear', 'hair drier', 'toothbrush']\n",
    "    id_list = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 27, 28, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 67, 70, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 84, 85, 86, 87, 88, 89, 90]\n",
    "    root = \"./data/COCO2017/val2017\"\n",
    "    annFile = \"./data/COCO2017/annotations/instances_val2017.json\"    \n",
    "    coco_gld = COCO(annFile) # coco\n",
    "    \n",
    "else:\n",
    "    print(\"invalid model number!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# COCO loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.transforms as transforms\n",
    "from torchvision.datasets import CocoDetection\n",
    "from pycocotools.coco import COCO\n",
    "import numpy as np\n",
    "import skimage.io as io\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "create dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=0.72s)\n",
      "creating index...\n",
      "index created!\n"
     ]
    }
   ],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "coco_dataset = CocoDetection(root=root, annFile=annFile, transform=transform)\n",
    "\n",
    "def collate_fn(batch):\n",
    "    return tuple(zip(*batch))\n",
    "\n",
    "# Create a DataLoader for your COCO dataset\n",
    "train_loader = DataLoader(coco_dataset, batch_size=32, shuffle=True, collate_fn=collate_fn)\n",
    "val_loader = DataLoader(coco_dataset, batch_size=1, shuffle=True, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "helper functions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "def xyxy2xywh(x):\n",
    "    # Convert nx4 boxes from [x1, y1, x2, y2] to [x, y, w, h] where xy1=top-left, xy2=bottom-right\n",
    "    y = x.clone() if isinstance(x, torch.Tensor) else np.copy(x)\n",
    "    y[..., 0] = (x[..., 0] + x[..., 2]) / 2  # x center\n",
    "    y[..., 1] = (x[..., 1] + x[..., 3]) / 2  # y center\n",
    "    y[..., 2] = x[..., 2] - x[..., 0]  # width\n",
    "    y[..., 3] = x[..., 3] - x[..., 1]  # height\n",
    "    return y\n",
    "\n",
    "def xywh2xyxy(x):\n",
    "    # Convert nx4 boxes from [x, y, w, h] to [x1, y1, x2, y2] where xy1=top-left, xy2=bottom-right\n",
    "    y = x.clone() if isinstance(x, torch.Tensor) else np.copy(x)\n",
    "    y[..., 0] = x[..., 0] - x[..., 2] / 2  # top left x\n",
    "    y[..., 1] = x[..., 1] - x[..., 3] / 2  # top left y\n",
    "    y[..., 2] = x[..., 0] + x[..., 2] / 2  # bottom right x\n",
    "    y[..., 3] = x[..., 1] + x[..., 3] / 2  # bottom right y\n",
    "    return y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Eval"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "inference and output json file containing prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/5000 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 63%|██████▎   | 3130/5000 [02:20<01:24, 22.13it/s]"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "from pycocotools.coco import COCO\n",
    "import json\n",
    "import cv2\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "from detect import detect_image\n",
    "from utils.utils import load_classes, rescale_boxes, non_max_suppression, print_environment_info\n",
    "from utils.transforms import Resize, DEFAULT_TRANSFORMS\n",
    "\n",
    "predictions = []\n",
    "img_size=416\n",
    "\n",
    "for i, data in enumerate(tqdm(val_loader)): # * ASSUME ONE IMAGE PER BATCH FOR NOW\n",
    "    with torch.no_grad():\n",
    "        images, targets = data\n",
    "        # print(targets)\n",
    "        if len(targets[0]) > 0:\n",
    "            model.eval()\n",
    "            # for target in targets[0]: print(f\"id {target['image_id']}, class {target['category_id']}, bbox {target['bbox']}\")\n",
    "            image_id = targets[0][0][\"image_id\"]\n",
    "            category_id = targets[0][0][\"category_id\"]\n",
    "            img = images[0].unsqueeze(0).to(device)\n",
    "            img_copy = img.clone().detach()\n",
    "            # print(img.shape) # torch.Size([1, 3, 375, 500])\n",
    "\n",
    "            # imageN = img.clone().detach()\n",
    "            # imageN = imageN.cpu().squeeze().permute(1, 2, 0).numpy() # * just for sanity check, output image. put the dim 3 at the back\n",
    "            # imageN = cv2.cvtColor(imageN, cv2.COLOR_RGB2BGR)\n",
    "            # # print(imageN.shape)\n",
    "            # cv2.imwrite(\"output/mygraph.jpg\", imageN*255) \n",
    "\n",
    "            if modelv == 2:\n",
    "                resized_tensor_image = torch.nn.functional.interpolate(img, size=(416, 416), mode='bilinear', align_corners=False)\n",
    "                # print(resized_tensor_image.shape)\n",
    "                # * default 0.66, 0.55, higher conf -> more strict, higher nms -> more iou needed -> more strict\n",
    "                boxes = filtered_boxes(model, resized_tensor_image, conf_thresh=0.5, nms_thresh=0.9, device=device)\n",
    "            elif modelv == 3:\n",
    "                img = img.cpu().squeeze().permute(1, 2, 0).numpy()\n",
    "                img = transforms.Compose([DEFAULT_TRANSFORMS, Resize(img_size)])((img, np.zeros((1, 5))))[0].unsqueeze(0).to(device)\n",
    "                boxes = model(img)\n",
    "                boxes = non_max_suppression(boxes, conf_thres=0.3, iou_thres=0.5)[0].numpy()\n",
    "                # print(boxes.shape, img_size, og_shape[2:])\n",
    "                # print(boxes)\n",
    "                \n",
    "                boxes = rescale_boxes(boxes, img_size, img_copy.shape[2:]) # rescale back to original proportions?\n",
    "                boxes = xyxy2xywh(boxes) # convert from coco to yolo\n",
    "                boxes[:,0] = boxes[:,0]/img_copy.shape[3]\n",
    "                boxes[:,1] = boxes[:,1]/img_copy.shape[2]\n",
    "                boxes[:,2] = boxes[:,2]/img_copy.shape[3]\n",
    "                boxes[:,3] = boxes[:,3]/img_copy.shape[2]\n",
    "                \n",
    "                boxes = np.concatenate((boxes[:, :5], boxes[:, 4:5], boxes[:, 5:]), axis=1)\n",
    "                boxes[:, 6] = boxes[:, 6].astype(int)\n",
    "            else: \n",
    "                print(\"invalid model num!\")\n",
    "            \n",
    "            # put into coco format of x_min,y_min, width, height, bbox_conf, cls\n",
    "            # yolo format is x_center, y_center, w, h, bbox_conf, cls_conf, cls\n",
    "            \n",
    "            # to_pil = transforms.ToPILImage() # * another sanity check\n",
    "            # pil_image = to_pil(img_copy.squeeze())\n",
    "            # pred_img = plot_boxes(pil_image, boxes, None, class_names)\n",
    "            # pil_image.save(\"./output/path_to_save_image.jpg\")\n",
    "            \n",
    "            for box in boxes:\n",
    "                x_center, y_center, w, h, bbox_conf, cls_conf, cls = box\n",
    "                x_min = max(0, (x_center - w / 2) * img_copy.shape[3])\n",
    "                y_min = max(0, (y_center - h / 2) * img_copy.shape[2])\n",
    "                width = min(img_copy.shape[3], w * img_copy.shape[3])\n",
    "                height = min(img_copy.shape[2], h * img_copy.shape[2])\n",
    "                # print(x_min,y_min, width, height, bbox_conf, cls)\n",
    "                predictions.append({\n",
    "                    'image_id': image_id,\n",
    "                    'category_id': id_list[int(cls)] if modelv == 3 else int(cls), # if using coco dataset with yolov3, convert class to category_id\n",
    "                    'bbox': [int(x_min), int(y_min), int(width), int(height)],\n",
    "                    'score': round(float(bbox_conf),2)\n",
    "                })\n",
    "            \n",
    "        else: # pics without labels\n",
    "            continue\n",
    "            # print(\"error\", targets) \n",
    "    # break\n",
    "\n",
    "# print(predictions)\n",
    "\n",
    "with open(f'./data/results/v{modelv}predictions.json', 'w') as f:\n",
    "    json.dump(predictions, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "compare gt and prediction\n",
    "\n",
    "install cocoapi: pip install git+https://github.com/philferriere/cocoapi.git#subdirectory=PythonAPI\n",
    "\n",
    "C:\\Users\\anton\\anaconda3\\Lib\\site-packages\\pycocotools\\cocoeval.py\n",
    "\n",
    "issue #1: object of type <class 'numpy.float64'> cannot be safely interpreted as an integer.\n",
    "\n",
    "fix: https://github.com/cocodataset/cocoapi/issues/356 at line 507,508, 518, 519\n",
    "\n",
    "https://github.com/cocodataset/cocoapi/pull/354/commits/6c3b394c07aed33fd83784a8bf8798059a1e9ae4#diff-b13be02996507277fdef1503fc74ab7ac7679a15450b60982222f8ca051bc8ee\n",
    "\n",
    "issue #2: module 'numpy' has no attribute 'float'\n",
    "\n",
    "fix: changed x.astype(float) in line 379,380 of cocoeval.py to np.float64(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading and preparing results...\n",
      "DONE (t=0.01s)\n",
      "creating index...\n",
      "index created!\n",
      "Running per image evaluation...\n",
      "Evaluate annotation type *bbox*\n",
      "DONE (t=0.25s).\n",
      "Accumulating evaluation results...\n",
      "DONE (t=0.11s).\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.128\n",
      " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.224\n",
      " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.138\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.089\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.179\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.242\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.273\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.273\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.132\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.355\n"
     ]
    }
   ],
   "source": [
    "from pycocotools.coco import COCO\n",
    "from pycocotools.cocoeval import COCOeval\n",
    "\n",
    "if modelv == 2:\n",
    "    coco_rst = coco_gld.loadRes('./data/results/v2predictions.json')\n",
    "elif modelv == 3:\n",
    "    coco_rst = coco_gld.loadRes('./data/results/v3predictions.json')\n",
    "cocoEval = COCOeval(coco_gld, coco_rst, iouType='bbox')\n",
    "cocoEval.evaluate()\n",
    "cocoEval.accumulate()\n",
    "cocoEval.summarize()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Untitled0.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
