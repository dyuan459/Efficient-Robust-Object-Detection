{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://github.com/miladlink/TinyYoloV2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "WVDh0TibRADG"
   },
   "outputs": [],
   "source": [
    "\n",
    "# import os\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import json\n",
    "import cv2\n",
    "from tqdm import tqdm\n",
    "# import skimage.io as io\n",
    "# import matplotlib.pyplot as plt\n",
    "from pycocotools.coco import COCO\n",
    "import torch\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.datasets.coco import CocoDetection\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from utils.YOLOv2 import *\n",
    "from models.YOLOv3 import load_model\n",
    "from attacks.FGSM import FGSM\n",
    "from detect import detect_image\n",
    "from utils.loss import compute_loss\n",
    "from utils.utils import load_classes, rescale_boxes, non_max_suppression, print_environment_info\n",
    "from utils.augmentations import TRANSFORM_TRAIN, TRANSFORM_VAL\n",
    "from utils.transforms import DEFAULT_TRANSFORMS, Resize, ResizeEval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "jlQVknSfeKt4"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelv = 3\n",
    "img_size=416\n",
    "\n",
    "if modelv == 2:\n",
    "    model = load_model_v2(weights = './weights/yolov2-tiny-voc.weights').to(device)\n",
    "    class_names = ['aeroplane', 'bicycle', 'bird', 'boat', 'bottle', 'bus', 'car', 'cat', 'chair', 'cow', 'diningtable', 'dog', 'horse', 'motorbike', 'person', 'pottedplant', 'sheep', 'sofa', 'train', 'TVmonitor'] \n",
    "    root_train = \"./data/VOC2007/JPEGImages\"\n",
    "    annFile_train = \"./data/VOC2007/annotations/train.json\"\n",
    "    root_val = \"./data/VOC2007/JPEGImages\"\n",
    "    annFile_val = \"./data/VOC2007/annotations/val.json\"\n",
    "    \n",
    "elif modelv == 3:\n",
    "    model = load_model(\"./config/yolov3.cfg\", \"./weights/yolov3.weights\")\n",
    "    class_names = ['person', 'bicycle', 'car', 'motorbike', 'aeroplane', 'bus', 'train', 'truck', 'boat', 'traffic light', 'fire hydrant', 'stop sign', 'parking meter', 'bench', 'bird', 'cat', 'dog', 'horse', 'sheep', 'cow', 'elephant', 'bear', 'zebra', 'giraffe', 'backpack', 'umbrella', 'handbag', 'tie', 'suitcase', 'frisbee', 'skis', 'snowboard', 'sports ball', 'kite', 'baseball bat', 'baseball glove', 'skateboard', 'surfboard', 'tennis racket', 'bottle', 'wine glass', 'cup', 'fork', 'knife', 'spoon', 'bowl', 'banana', 'apple', 'sandwich', 'orange', 'broccoli', 'carrot', 'hot dog', 'pizza', 'donut', 'cake', 'chair', 'sofa', 'pottedplant', 'bed', 'diningtable', 'toilet', 'tvmonitor', 'laptop', 'mouse', 'remote', 'keyboard', 'cell phone', 'microwave', 'oven', 'toaster', 'sink', 'refrigerator', 'book', 'clock', 'vase', 'scissors', 'teddy bear', 'hair drier', 'toothbrush']\n",
    "    id_list = np.array([1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 27, 28, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 67, 70, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 84, 85, 86, 87, 88, 89, 90])\n",
    "    root_train = \"./data/COCO2017/train2017\"\n",
    "    annFile_train = \"./data/COCO2017/annotations/instances_train2017_modified.json\"\n",
    "    root_val = \"./data/COCO2017/val2017\"\n",
    "    annFile_val = \"./data/COCO2017/annotations/instances_val2017_modified.json\"\n",
    "    \n",
    "else:\n",
    "    print(\"invalid model number!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# COCO loader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "create dataloader (make different train and val later)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=0.24s)\n",
      "creating index...\n",
      "index created!\n",
      "loading annotations into memory...\n",
      "Done (t=0.23s)\n",
      "creating index...\n",
      "index created!\n"
     ]
    }
   ],
   "source": [
    "# coco_dataset_train = CocoDetection(root=root_train, annFile=annFile_train, transform=TRANSFORM_TRAIN_IMG, target_transform=TRANSFORM_TRAIN_TARGET)\n",
    "coco_dataset_val = CocoDetection(root=root_val, annFile=annFile_val, transforms=TRANSFORM_VAL)\n",
    "coco_dataset_eval = CocoDetection(root=root_val, annFile=annFile_val, transform=transforms.Compose([transforms.ToTensor(),]))\n",
    "\n",
    "def collate_fn(batch):\n",
    "    return tuple(zip(*batch))\n",
    "\n",
    "# Create a DataLoader for your COCO dataset\n",
    "# train_loader = DataLoader(coco_dataset_train, batch_size=32, shuffle=True, collate_fn=collate_fn)\n",
    "val_loader = DataLoader(coco_dataset_val, batch_size=1, shuffle=True, collate_fn=collate_fn)\n",
    "cocoeval_loader = DataLoader(coco_dataset_eval, batch_size=1, shuffle=True, collate_fn=collate_fn)\n",
    "# images, annotations = next(iter(val_loader))\n",
    "# np.set_printoptions(linewidth=500)\n",
    "# np.set_printoptions(suppress=True)\n",
    "# print(\"dataloader out\")\n",
    "# print(annotations[0].numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "helper functions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def xyxy2xywh(x):\n",
    "    # Convert nx4 boxes from [x1, y1, x2, y2] to [x, y, w, h] where xy1=top-left, xy2=bottom-right\n",
    "    y = x.clone() if isinstance(x, torch.Tensor) else np.copy(x)\n",
    "    y[..., 0] = (x[..., 0] + x[..., 2]) / 2  # x center\n",
    "    y[..., 1] = (x[..., 1] + x[..., 3]) / 2  # y center\n",
    "    y[..., 2] = x[..., 2] - x[..., 0]  # width\n",
    "    y[..., 3] = x[..., 3] - x[..., 1]  # height\n",
    "    return y\n",
    "\n",
    "def xywh2xyxy(x):\n",
    "    # Convert nx4 boxes from [x, y, w, h] to [x1, y1, x2, y2] where xy1=top-left, xy2=bottom-right\n",
    "    y = x.clone() if isinstance(x, torch.Tensor) else np.copy(x)\n",
    "    y[..., 0] = x[..., 0] - x[..., 2] / 2  # top left x\n",
    "    y[..., 1] = x[..., 1] - x[..., 3] / 2  # top left y\n",
    "    y[..., 2] = x[..., 0] + x[..., 2] / 2  # bottom right x\n",
    "    y[..., 3] = x[..., 1] + x[..., 3] / 2  # bottom right y\n",
    "    return y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Eval for COCOeval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5000/5000 [02:38<00:00, 31.55it/s]\n"
     ]
    }
   ],
   "source": [
    "predictions = []\n",
    "img_size=416\n",
    "\n",
    "for i, data in enumerate(tqdm(cocoeval_loader)): # * ASSUME ONE IMAGE PER BATCH FOR NOW\n",
    "    with torch.no_grad():\n",
    "        images, targets = data\n",
    "        # print(targets[0])\n",
    "        if len(targets[0]) > 0:\n",
    "            model.eval()\n",
    "            \n",
    "            # * getting necessary values\n",
    "            # for target in targets[0]: print(f\"id {target['image_id']}, class {target['category_id']}, bbox {target['bbox']}\")\n",
    "            image_id = targets[0][0][\"image_id\"]\n",
    "            category_id = targets[0][0][\"category_id\"]\n",
    "            img = images[0].unsqueeze(0).to(device)\n",
    "            img_copy = img.clone().detach()\n",
    "            # print(img.shape) # torch.Size([1, 3, 375, 500])\n",
    "            \n",
    "            # * turn target into 2d numpy matrix for loss calc\n",
    "            targetsMatrix = np.zeros((len(targets[0]), 6))\n",
    "            for i, d in enumerate(targets[0]):\n",
    "                targetsMatrix[i, 0] = d['image_id']\n",
    "                targetsMatrix[i, 1] = d['category_id']\n",
    "                targetsMatrix[i, 2:6] = d['bbox']\n",
    "            # np.set_printoptions(suppress=True)\n",
    "            # print(targetsMatrix)\n",
    "\n",
    "            # * just for sanity check, output image. put the dim 3 at the back\n",
    "            # imageN = img.clone().detach()\n",
    "            # imageN = imageN.cpu().squeeze().permute(1, 2, 0).numpy() \n",
    "            # imageN = cv2.cvtColor(imageN, cv2.COLOR_RGB2BGR)\n",
    "            # # print(imageN.shape)\n",
    "            # cv2.imwrite(\"output/mygraph.jpg\", imageN*255) \n",
    "\n",
    "            # * put images into model and get bboxes\n",
    "            if modelv == 2:\n",
    "                resized_tensor_image = torch.nn.functional.interpolate(img, size=(416, 416), mode='bilinear', align_corners=False)\n",
    "                # print(resized_tensor_image.shape)\n",
    "                # * default 0.66, 0.55, higher conf -> more strict, higher nms -> more iou needed -> more strict\n",
    "                boxes = filtered_boxes(model, resized_tensor_image, conf_thresh=0.5, nms_thresh=0.9, device=device)\n",
    "                boxes = np.delete(boxes, 5, axis=1) # x_center, y_center, w, h, bbox_conf, cls_conf, cls -> delete cls_conf\n",
    "            elif modelv == 3:\n",
    "                img = img.cpu().squeeze().permute(1, 2, 0).numpy()\n",
    "                img = transforms.Compose([DEFAULT_TRANSFORMS, ResizeEval(img_size)])((img, np.zeros((1, 5))))[0].unsqueeze(0).to(device)\n",
    "                boxes = model(img)\n",
    "                boxes = non_max_suppression(boxes, conf_thres=0.3, iou_thres=0.5)[0].numpy()\n",
    "                # print(boxes.shape, img_size, og_shape[2:])\n",
    "                # print(boxes)\n",
    "                \n",
    "                boxes = rescale_boxes(boxes, img_size, img_copy.shape[2:]) # rescale back to original proportions?\n",
    "                boxes = xyxy2xywh(boxes) # convert from coco to yolo\n",
    "                boxes[:,0] = boxes[:,0]/img_copy.shape[3]\n",
    "                boxes[:,1] = boxes[:,1]/img_copy.shape[2]\n",
    "                boxes[:,2] = boxes[:,2]/img_copy.shape[3]\n",
    "                boxes[:,3] = boxes[:,3]/img_copy.shape[2]\n",
    "            else: \n",
    "                print(\"invalid model num!\")\n",
    "                \n",
    "            \n",
    "            # * another sanity check\n",
    "            # to_pil = transforms.ToPILImage() \n",
    "            # pil_image = to_pil(img_copy.squeeze())\n",
    "            # pred_img = plot_boxes(pil_image, boxes, None, class_names)\n",
    "            # pil_image.save(\"./output/path_to_save_image.jpg\")\n",
    "            \n",
    "            # * put into coco format of x_min,y_min, width, height, bbox_conf, cls\n",
    "            # yolo format is x_center, y_center, w, h, bbox_conf, cls_conf, cls\n",
    "            for box in boxes:\n",
    "                x_center, y_center, w, h, conf, cls = box\n",
    "                x_min = max(0, (x_center - w / 2) * img_copy.shape[3])\n",
    "                y_min = max(0, (y_center - h / 2) * img_copy.shape[2])\n",
    "                width = min(img_copy.shape[3], w * img_copy.shape[3])\n",
    "                height = min(img_copy.shape[2], h * img_copy.shape[2])\n",
    "                # print(x_min,y_min, width, height, bbox_conf, cls)\n",
    "                predictions.append({\n",
    "                    'image_id': image_id,\n",
    "                    'category_id': int(id_list[int(cls)]) if modelv == 3 else int(cls),\n",
    "                    'bbox': [int(x_min), int(y_min), int(width), int(height)],\n",
    "                    'score': round(float(conf),2)\n",
    "                })\n",
    "            \n",
    "        else: # pics without labels\n",
    "            continue\n",
    "            # print(\"error\", targets) \n",
    "    # break\n",
    "\n",
    "with open(f'./data/results/v{modelv}predictions.json', 'w') as f:\n",
    "    json.dump(predictions, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Eval for loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "inference and output json file containing prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/5000 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'image_id': 173302, 'category_id': 62, 'bbox': [252, 197, 46, 99], 'score': 1.0}, {'image_id': 173302, 'category_id': 62, 'bbox': [156, 250, 75, 53], 'score': 0.99}, {'image_id': 173302, 'category_id': 62, 'bbox': [88, 215, 41, 21], 'score': 0.99}, {'image_id': 173302, 'category_id': 62, 'bbox': [219, 196, 32, 72], 'score': 0.96}, {'image_id': 173302, 'category_id': 62, 'bbox': [194, 195, 23, 49], 'score': 0.95}, {'image_id': 173302, 'category_id': 1, 'bbox': [240, 165, 26, 32], 'score': 0.93}, {'image_id': 173302, 'category_id': 82, 'bbox': [119, 152, 27, 72], 'score': 0.81}, {'image_id': 173302, 'category_id': 1, 'bbox': [225, 162, 18, 33], 'score': 0.73}, {'image_id': 173302, 'category_id': 67, 'bbox': [65, 227, 166, 70], 'score': 0.71}, {'image_id': 173302, 'category_id': 62, 'bbox': [66, 220, 24, 16], 'score': 0.54}, {'image_id': 173302, 'category_id': 82, 'bbox': [114, 154, 23, 57], 'score': 0.51}, {'image_id': 173302, 'category_id': 62, 'bbox': [64, 221, 8, 11], 'score': 0.5}, {'image_id': 173302, 'category_id': 86, 'bbox': [95, 198, 6, 9], 'score': 0.45}, {'image_id': 173302, 'category_id': 79, 'bbox': [270, 174, 31, 22], 'score': 0.36}, {'image_id': 173302, 'category_id': 44, 'bbox': [95, 198, 6, 9], 'score': 0.34}, {'image_id': 173302, 'category_id': 44, 'bbox': [87, 199, 5, 7], 'score': 0.34}, {'image_id': 173302, 'category_id': 78, 'bbox': [270, 174, 31, 22], 'score': 0.32}]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "predictions = []\n",
    "\n",
    "for i, data in enumerate(tqdm(val_loader)): # * ASSUME ONE IMAGE PER BATCH FOR NOW\n",
    "    with torch.no_grad():\n",
    "        images, targets = data\n",
    "        if len(targets[0]) > 0:\n",
    "            model.eval()\n",
    "            # print(targets)\n",
    "            # * getting necessary values\n",
    "            # for target in targets[0]: print(f\"id {target['image_id']}, class {target['category_id']}, bbox {target['bbox']}\")\n",
    "            image_id = int(targets[0][0][0])\n",
    "            category_id = targets[0][:, 0]\n",
    "            img = images[0].unsqueeze(0).to(device)\n",
    "            img_copy = img.clone().detach()\n",
    "            # print(img.shape) # torch.Size([1, 3, 375, 500])\n",
    "\n",
    "            # * just for sanity check, output image. put the dim 3 at the back\n",
    "            # imageN = img.clone().detach()\n",
    "            # imageN = imageN.cpu().squeeze().permute(1, 2, 0).numpy() \n",
    "            # imageN = cv2.cvtColor(imageN, cv2.COLOR_RGB2BGR)\n",
    "            # # print(imageN.shape)\n",
    "            # cv2.imwrite(\"output/mygraph.jpg\", imageN*255) \n",
    "\n",
    "            # * put images into model and get bboxes\n",
    "            if modelv == 2:\n",
    "                resized_tensor_image = torch.nn.functional.interpolate(img, size=(416, 416), mode='bilinear', align_corners=False)\n",
    "                # print(resized_tensor_image.shape)\n",
    "                # * default 0.66, 0.55, higher conf -> more strict, higher nms -> more iou needed -> more strict\n",
    "                boxes = filtered_boxes(model, resized_tensor_image, conf_thresh=0.5, nms_thresh=0.9, device=device)\n",
    "                if boxes != []: boxes = np.delete(boxes, 5, axis=1) # x_center, y_center, w, h, bbox_conf, cls_conf, cls -> delete cls_conf\n",
    "            \n",
    "            elif modelv == 3:\n",
    "                boxes = model(img)\n",
    "                boxes = non_max_suppression(boxes, conf_thres=0.3, iou_thres=0.5)[0].numpy()\n",
    "                boxes = rescale_boxes(boxes, img_size, img_copy.shape[2:]) # rescale back to original, for cocoeval\n",
    "                boxes = xyxy2xywh(boxes) # nms returns xyxy, convert to yolo\n",
    "                boxes[:,0] = boxes[:,0]/img_copy.shape[3]\n",
    "                boxes[:,1] = boxes[:,1]/img_copy.shape[2]\n",
    "                boxes[:,2] = boxes[:,2]/img_copy.shape[3]\n",
    "                boxes[:,3] = boxes[:,3]/img_copy.shape[2]\n",
    "            \n",
    "                # boxes[:, 5] = id_list[boxes[:, 5].astype(int)] # convert class to category_id \n",
    "            else: \n",
    "                print(\"invalid model num!\")\n",
    "                \n",
    "            \n",
    "            # # * another sanity check\n",
    "            # to_pil = transforms.ToPILImage() \n",
    "            # pil_image = to_pil(img_copy.squeeze())\n",
    "            # pred_img = plot_boxes(pil_image, boxes, None, class_names)\n",
    "            # pil_image.save(\"./output/path_to_save_image.jpg\")\n",
    "            \n",
    "            # * put into coco format of x_min,y_min, width, height, bbox_conf, cls\n",
    "            # yolo format is x_center, y_center, w, h, bbox_conf, cls_conf, cls\n",
    "            for box in boxes:\n",
    "                x_center, y_center, w, h, conf, cls = box\n",
    "                x_min = max(0, (x_center - w / 2) * img_copy.shape[3])\n",
    "                y_min = max(0, (y_center - h / 2) * img_copy.shape[2])\n",
    "                width = min(img_copy.shape[3], w * img_copy.shape[3])\n",
    "                height = min(img_copy.shape[2], h * img_copy.shape[2])\n",
    "                # print(x_min,y_min, width, height, bbox_conf, cls)\n",
    "                predictions.append({\n",
    "                    'image_id': int(image_id),\n",
    "                    'category_id': int(id_list[int(cls)]) if modelv == 3 else int(cls),\n",
    "                    'bbox': [int(x_min), int(y_min), int(width), int(height)],\n",
    "                    'score': round(float(conf),2)\n",
    "                })\n",
    "            \n",
    "        else: # pics without labels\n",
    "            continue\n",
    "            # print(\"error\", targets) \n",
    "    break\n",
    "print(predictions)\n",
    "\n",
    "with open(f'./data/results/v{modelv}predictions.json', 'w') as f:\n",
    "    json.dump(predictions, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# compare gt and prediction\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=0.16s)\n",
      "creating index...\n",
      "index created!\n",
      "Loading and preparing results...\n",
      "DONE (t=0.28s)\n",
      "creating index...\n",
      "index created!\n",
      "Running per image evaluation...\n",
      "Evaluate annotation type *bbox*\n",
      "DONE (t=10.97s).\n",
      "Accumulating evaluation results...\n",
      "DONE (t=1.66s).\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.328\n",
      " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.560\n",
      " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.346\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.141\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.360\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.508\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.266\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.372\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.376\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.164\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.407\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.580\n"
     ]
    }
   ],
   "source": [
    "from pycocotools.coco import COCO\n",
    "from pycocotools.cocoeval import COCOeval\n",
    "        \n",
    "coco_gld = COCO(annFile_val) # coco\n",
    "if modelv == 2:\n",
    "    coco_rst = coco_gld.loadRes('./data/results/v2predictions.json')\n",
    "elif modelv == 3:\n",
    "    coco_rst = coco_gld.loadRes('./data/results/v3predictions.json')\n",
    "cocoEval = COCOeval(coco_gld, coco_rst, iouType='bbox')\n",
    "cocoEval.evaluate()\n",
    "cocoEval.accumulate()\n",
    "cocoEval.summarize()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Untitled0.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
