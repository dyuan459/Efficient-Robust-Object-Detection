{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://github.com/miladlink/TinyYoloV2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "WVDh0TibRADG"
   },
   "outputs": [],
   "source": [
    "\n",
    "# import os\n",
    "import time\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import json\n",
    "import cv2\n",
    "from tqdm import tqdm\n",
    "# import skimage.io as io\n",
    "# import matplotlib.pyplot as plt\n",
    "from pycocotools.coco import COCO\n",
    "import torch\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.datasets.coco import CocoDetection\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from utils.YOLOv2 import *\n",
    "from models.YOLOv3 import load_model\n",
    "from attacks.FGSM import FGSM\n",
    "from attacks.PGD import PGD\n",
    "from attacks.CW import CW\n",
    "from detect import detect_image\n",
    "from utils.loss import compute_loss\n",
    "from utils.utils import load_classes, rescale_boxes, non_max_suppression, print_environment_info\n",
    "from utils.augmentations import TRANSFORM_TRAIN, TRANSFORM_VAL\n",
    "from utils.transforms import DEFAULT_TRANSFORMS, Resize, ResizeEval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "jlQVknSfeKt4"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# helper functions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def xyxy2xywh(x):\n",
    "    # Convert nx4 boxes from [x1, y1, x2, y2] to [x, y, w, h] where xy1=top-left, xy2=bottom-right\n",
    "    y = x.clone() if isinstance(x, torch.Tensor) else np.copy(x)\n",
    "    y[..., 0] = (x[..., 0] + x[..., 2]) / 2  # x center\n",
    "    y[..., 1] = (x[..., 1] + x[..., 3]) / 2  # y center\n",
    "    y[..., 2] = x[..., 2] - x[..., 0]  # width\n",
    "    y[..., 3] = x[..., 3] - x[..., 1]  # height\n",
    "    return y\n",
    "\n",
    "def xywh2xyxy(x):\n",
    "    # Convert nx4 boxes from [x, y, w, h] to [x1, y1, x2, y2] where xy1=top-left, xy2=bottom-right\n",
    "    y = x.clone() if isinstance(x, torch.Tensor) else np.copy(x)\n",
    "    y[..., 0] = x[..., 0] - x[..., 2] / 2  # top left x\n",
    "    y[..., 1] = x[..., 1] - x[..., 3] / 2  # top left y\n",
    "    y[..., 2] = x[..., 0] + x[..., 2] / 2  # bottom right x\n",
    "    y[..., 3] = x[..., 1] + x[..., 3] / 2  # bottom right y\n",
    "    return y\n",
    "\n",
    "def yolo2json(boxes, img_copy, image_id):\n",
    "    # * put into coco format of x_min,y_min, width, height, bbox_conf, cls\n",
    "    # yolo format is x_center, y_center, w, h, bbox_conf, cls_conf, cls\n",
    "    predictions = []\n",
    "    for box in boxes:\n",
    "        x_center, y_center, w, h, conf, cls = box\n",
    "        x_min = max(0, (x_center - w / 2) * img_copy.shape[3])\n",
    "        y_min = max(0, (y_center - h / 2) * img_copy.shape[2])\n",
    "        width = min(img_copy.shape[3], w * img_copy.shape[3])\n",
    "        height = min(img_copy.shape[2], h * img_copy.shape[2])\n",
    "        # print(x_min,y_min, width, height, bbox_conf, cls)\n",
    "        predictions.append({\n",
    "            'image_id': image_id,\n",
    "            'category_id': int(id_list[int(cls)]) if modelv == 3 else int(cls),\n",
    "            'bbox': [int(x_min), int(y_min), int(width), int(height)],\n",
    "            'score': round(float(conf),2)\n",
    "        })\n",
    "    return predictions\n",
    "\n",
    "def nms2yolo(boxes, img_copy):\n",
    "    boxes = xyxy2xywh(boxes) # convert from coco to yolo: nms returns nx6 (x1, y1, x2, y2, conf, cls), change to center\n",
    "    boxes[:,0] = boxes[:,0]/img_copy.shape[3]\n",
    "    boxes[:,1] = boxes[:,1]/img_copy.shape[2]\n",
    "    boxes[:,2] = boxes[:,2]/img_copy.shape[3]\n",
    "    boxes[:,3] = boxes[:,3]/img_copy.shape[2]\n",
    "    return boxes\n",
    "\n",
    "def saveImageWithBoxes(images, boxes, class_names, fileName):  \n",
    "    to_pil = transforms.ToPILImage() \n",
    "    pil_image = to_pil(images.squeeze())\n",
    "    pred_img = plot_boxes(pil_image, boxes, None, class_names)\n",
    "    pred_img.save(fileName)\n",
    "    \n",
    "def saveImage(img):\n",
    "    # * just for sanity check, output image. put the dim 3 at the back\n",
    "    imageN = img.clone().detach()\n",
    "    imageN = imageN.cpu().squeeze().permute(1, 2, 0).numpy() \n",
    "    imageN = cv2.cvtColor(imageN, cv2.COLOR_RGB2BGR)\n",
    "    # print(imageN.shape)\n",
    "    cv2.imwrite(\"output/mygraph.jpg\", imageN*255) \n",
    "    \n",
    "def getOneIter(dataloader):\n",
    "    images, annotations = next(iter(dataloader))\n",
    "    np.set_printoptions(linewidth=500)\n",
    "    np.set_printoptions(suppress=True)\n",
    "    print(\"dataloader out\")\n",
    "    print(annotations[0].numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelv = 3\n",
    "img_size=416\n",
    "\n",
    "if modelv == 2:\n",
    "    model = load_model_v2(weights = './weights/yolov2-tiny-voc.weights').to(device)\n",
    "    class_names = ['aeroplane', 'bicycle', 'bird', 'boat', 'bottle', 'bus', 'car', 'cat', 'chair', 'cow', 'diningtable', 'dog', 'horse', 'motorbike', 'person', 'pottedplant', 'sheep', 'sofa', 'train', 'TVmonitor'] \n",
    "    root_train = \"./data/VOC2007/JPEGImages\"\n",
    "    annFile_train = \"./data/VOC2007/annotations/train.json\"\n",
    "    root_val = \"./data/VOC2007/JPEGImages\"\n",
    "    annFile_val = \"./data/VOC2007/annotations/val.json\"\n",
    "    \n",
    "elif modelv == 3:\n",
    "    model = load_model(\"./config/yolov3.cfg\", \"./weights/yolov3.weights\")\n",
    "    class_names = ['person', 'bicycle', 'car', 'motorbike', 'aeroplane', 'bus', 'train', 'truck', 'boat', 'traffic light', 'fire hydrant', 'stop sign', 'parking meter', 'bench', 'bird', 'cat', 'dog', 'horse', 'sheep', 'cow', 'elephant', 'bear', 'zebra', 'giraffe', 'backpack', 'umbrella', 'handbag', 'tie', 'suitcase', 'frisbee', 'skis', 'snowboard', 'sports ball', 'kite', 'baseball bat', 'baseball glove', 'skateboard', 'surfboard', 'tennis racket', 'bottle', 'wine glass', 'cup', 'fork', 'knife', 'spoon', 'bowl', 'banana', 'apple', 'sandwich', 'orange', 'broccoli', 'carrot', 'hot dog', 'pizza', 'donut', 'cake', 'chair', 'sofa', 'pottedplant', 'bed', 'diningtable', 'toilet', 'tvmonitor', 'laptop', 'mouse', 'remote', 'keyboard', 'cell phone', 'microwave', 'oven', 'toaster', 'sink', 'refrigerator', 'book', 'clock', 'vase', 'scissors', 'teddy bear', 'hair drier', 'toothbrush']\n",
    "    id_list = np.array([1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 27, 28, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 67, 70, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 84, 85, 86, 87, 88, 89, 90])\n",
    "    root_train = \"./data/COCO2017/train2017\"\n",
    "    annFile_train = \"./data/COCO2017/annotations/instances_train2017_modified.json\"\n",
    "    root_val = \"./data/COCO2017/val2017\"\n",
    "    annFile_val = \"./data/COCO2017/annotations/instances_val2017_modified.json\"\n",
    "    \n",
    "else:\n",
    "    print(\"invalid model number!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# COCO loader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "create dataloader (make different train and val later)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=0.28s)\n",
      "creating index...\n",
      "index created!\n",
      "loading annotations into memory...\n",
      "Done (t=0.16s)\n",
      "creating index...\n",
      "index created!\n"
     ]
    }
   ],
   "source": [
    "# coco_dataset_train = CocoDetection(root=root_train, annFile=annFile_train, transform=TRANSFORM_TRAIN_IMG, target_transform=TRANSFORM_TRAIN_TARGET)\n",
    "coco_dataset_val = CocoDetection(root=root_val, annFile=annFile_val, transforms=TRANSFORM_VAL)\n",
    "coco_dataset_eval = CocoDetection(root=root_val, annFile=annFile_val, transform=transforms.Compose([transforms.ToTensor(),]))\n",
    "\n",
    "def collate_fn(batch):\n",
    "    return tuple(zip(*batch))\n",
    "\n",
    "# Create a DataLoader for your COCO dataset\n",
    "train_loader = DataLoader(coco_dataset_val, batch_size=4, shuffle=True, collate_fn=collate_fn) # multiple images per batch\n",
    "val_loader = DataLoader(coco_dataset_val, batch_size=1, shuffle=True, collate_fn=collate_fn) # one per batch\n",
    "cocoeval_loader = DataLoader(coco_dataset_eval, batch_size=1, shuffle=True, collate_fn=collate_fn) # original images without transformatios\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataloader out\n",
      "(tensor([[4.5594e+05, 0.0000e+00, 5.1794e-01, 6.0730e+01, 2.1356e-01, 3.4440e+01,\n",
      "         4.8000e+02, 6.4000e+02],\n",
      "        [4.5594e+05, 0.0000e+00, 5.3548e-01, 1.2261e+02, 3.7903e-01, 6.2280e+01,\n",
      "         4.8000e+02, 6.4000e+02],\n",
      "        [4.5594e+05, 0.0000e+00, 1.8708e-01, 7.6283e+01, 2.1067e-01, 6.9393e+01,\n",
      "         4.8000e+02, 6.4000e+02],\n",
      "        [4.5594e+05, 0.0000e+00, 1.5991e-01, 8.5460e+01, 7.6594e-02, 1.3047e+01,\n",
      "         4.8000e+02, 6.4000e+02],\n",
      "        [4.5594e+05, 0.0000e+00, 3.4697e-01, 9.0523e+01, 1.1680e-01, 1.6707e+01,\n",
      "         4.8000e+02, 6.4000e+02],\n",
      "        [4.5594e+05, 0.0000e+00, 7.6845e-01, 1.0581e+02, 7.5500e-02, 1.2887e+01,\n",
      "         4.8000e+02, 6.4000e+02],\n",
      "        [4.5594e+05, 0.0000e+00, 1.2364e-01, 1.3198e+02, 1.3294e-01, 2.8613e+01,\n",
      "         4.8000e+02, 6.4000e+02],\n",
      "        [4.5594e+05, 0.0000e+00, 9.9656e-02, 9.4743e+01, 2.1094e-02, 7.2100e+00,\n",
      "         4.8000e+02, 6.4000e+02],\n",
      "        [4.5594e+05, 0.0000e+00, 5.2089e-01, 1.1880e+02, 3.9102e-01, 6.7867e+01,\n",
      "         4.8000e+02, 6.4000e+02],\n",
      "        [4.5594e+05, 0.0000e+00, 6.2359e-02, 9.8577e+01, 1.2472e-01, 1.9417e+01,\n",
      "         4.8000e+02, 6.4000e+02],\n",
      "        [4.5594e+05, 0.0000e+00, 3.8248e-01, 1.0248e+02, 1.0234e-02, 2.0167e+00,\n",
      "         4.8000e+02, 6.4000e+02],\n",
      "        [4.5594e+05, 0.0000e+00, 9.3623e-01, 2.9767e+01, 3.1609e-02, 9.2600e+00,\n",
      "         4.8000e+02, 6.4000e+02],\n",
      "        [4.5594e+05, 0.0000e+00, 9.4711e-02, 1.3204e+02, 1.8942e-01, 3.6267e+01,\n",
      "         4.8000e+02, 6.4000e+02],\n",
      "        [4.5594e+05, 0.0000e+00, 7.0539e-02, 9.3620e+01, 1.4108e-01, 5.6023e+01,\n",
      "         4.8000e+02, 6.4000e+02]], dtype=torch.float64), tensor([[8.5376e+04, 0.0000e+00, 3.8464e-01, 1.1057e+02, 1.5670e-01, 1.8020e+01,\n",
      "         6.4000e+02, 4.8000e+02],\n",
      "        [8.5376e+04, 0.0000e+00, 1.8463e-01, 1.0698e+02, 2.4228e-01, 1.8210e+01,\n",
      "         6.4000e+02, 4.8000e+02],\n",
      "        [8.5376e+04, 0.0000e+00, 1.2950e-01, 1.2361e+02, 1.3708e-01, 3.0200e+01,\n",
      "         6.4000e+02, 4.8000e+02],\n",
      "        [8.5376e+04, 0.0000e+00, 3.3469e-01, 1.0637e+02, 1.0677e-01, 7.9733e+00,\n",
      "         6.4000e+02, 4.8000e+02],\n",
      "        [8.5376e+04, 0.0000e+00, 4.4725e-01, 1.0797e+02, 1.1422e-01, 3.7167e+00,\n",
      "         6.4000e+02, 4.8000e+02],\n",
      "        [8.5376e+04, 0.0000e+00, 1.4653e-01, 1.0240e+02, 8.9547e-02, 6.7033e+00,\n",
      "         6.4000e+02, 4.8000e+02],\n",
      "        [8.5376e+04, 0.0000e+00, 2.0321e-01, 1.3773e+02, 4.0643e-01, 7.0760e+01,\n",
      "         6.4000e+02, 4.8000e+02],\n",
      "        [8.5376e+04, 0.0000e+00, 1.2500e-01, 1.0246e+02, 2.8875e-02, 1.9353e+01,\n",
      "         6.4000e+02, 4.8000e+02],\n",
      "        [8.5376e+04, 0.0000e+00, 5.0659e-01, 1.1068e+02, 5.5531e-02, 4.8900e+00,\n",
      "         6.4000e+02, 4.8000e+02],\n",
      "        [8.5376e+04, 0.0000e+00, 7.8638e-01, 1.0660e+02, 8.8625e-02, 3.1987e+01,\n",
      "         6.4000e+02, 4.8000e+02],\n",
      "        [8.5376e+04, 0.0000e+00, 5.0162e-01, 1.0528e+02, 3.1428e-01, 3.8790e+01,\n",
      "         6.4000e+02, 4.8000e+02]], dtype=torch.float64), tensor([[4.5832e+05, 0.0000e+00, 5.3495e-01, 5.6087e+01, 1.7078e-02, 8.8200e+00,\n",
      "         4.2700e+02, 6.4000e+02],\n",
      "        [4.5832e+05, 0.0000e+00, 2.3791e-01, 5.8383e+01, 1.8000e-02, 8.1233e+00,\n",
      "         4.2700e+02, 6.4000e+02],\n",
      "        [4.5832e+05, 0.0000e+00, 4.6294e-01, 8.6910e+01, 1.1497e-01, 6.6513e+01,\n",
      "         4.2700e+02, 6.4000e+02],\n",
      "        [4.5832e+05, 0.0000e+00, 4.2487e-01, 9.5587e+01, 4.2547e-02, 1.8743e+01,\n",
      "         4.2700e+02, 6.4000e+02],\n",
      "        [4.5832e+05, 0.0000e+00, 3.1872e-01, 9.4877e+01, 4.1047e-02, 2.0263e+01,\n",
      "         4.2700e+02, 6.4000e+02],\n",
      "        [4.5832e+05, 0.0000e+00, 4.0316e-01, 9.6280e+01, 2.3578e-02, 1.8303e+01,\n",
      "         4.2700e+02, 6.4000e+02],\n",
      "        [4.5832e+05, 0.0000e+00, 3.7136e-01, 9.7127e+01, 3.2375e-02, 1.5097e+01,\n",
      "         4.2700e+02, 6.4000e+02],\n",
      "        [4.5832e+05, 0.0000e+00, 7.5172e-01, 9.6403e+01, 1.5078e-02, 2.9300e+00,\n",
      "         4.2700e+02, 6.4000e+02],\n",
      "        [4.5832e+05, 0.0000e+00, 6.4783e-01, 9.6767e+01, 1.8094e-02, 4.3733e+00,\n",
      "         4.2700e+02, 6.4000e+02],\n",
      "        [4.5832e+05, 0.0000e+00, 4.8412e-01, 1.4495e+02, 4.4844e-02, 6.8500e+00,\n",
      "         4.2700e+02, 6.4000e+02],\n",
      "        [4.5832e+05, 0.0000e+00, 4.2453e-01, 1.0220e+02, 2.8109e-02, 1.1727e+01,\n",
      "         4.2700e+02, 6.4000e+02],\n",
      "        [4.5832e+05, 0.0000e+00, 9.1492e-01, 9.3543e+01, 4.4875e-02, 8.8467e+00,\n",
      "         4.2700e+02, 6.4000e+02],\n",
      "        [4.5832e+05, 0.0000e+00, 8.6012e-01, 1.0381e+02, 1.3987e-01, 6.1690e+01,\n",
      "         4.2700e+02, 6.4000e+02],\n",
      "        [4.5832e+05, 0.0000e+00, 7.5784e-01, 1.0126e+02, 2.4205e-01, 4.8603e+01,\n",
      "         4.2700e+02, 6.4000e+02],\n",
      "        [4.5832e+05, 0.0000e+00, 2.7988e-02, 1.0479e+02, 5.5977e-02, 7.1653e+01,\n",
      "         4.2700e+02, 6.4000e+02],\n",
      "        [4.5832e+05, 0.0000e+00, 7.0034e-01, 9.6560e+01, 2.1308e-01, 4.2433e+01,\n",
      "         4.2700e+02, 6.4000e+02],\n",
      "        [4.5832e+05, 0.0000e+00, 6.6283e-01, 9.9610e+01, 9.5281e-02, 3.2020e+01,\n",
      "         4.2700e+02, 6.4000e+02],\n",
      "        [4.5832e+05, 0.0000e+00, 5.9252e-01, 1.0148e+02, 6.8406e-02, 2.2377e+01,\n",
      "         4.2700e+02, 6.4000e+02],\n",
      "        [4.5832e+05, 0.0000e+00, 7.4199e-02, 9.0657e+01, 1.4840e-01, 5.5460e+01,\n",
      "         4.2700e+02, 6.4000e+02],\n",
      "        [4.5832e+05, 0.0000e+00, 5.4160e-02, 1.0600e+02, 1.0832e-01, 6.0953e+01,\n",
      "         4.2700e+02, 6.4000e+02],\n",
      "        [4.5832e+05, 0.0000e+00, 6.3825e-01, 9.8367e+01, 1.1288e-01, 2.8507e+01,\n",
      "         4.2700e+02, 6.4000e+02],\n",
      "        [4.5832e+05, 0.0000e+00, 5.5739e-01, 1.0077e+02, 1.0594e-01, 2.0100e+01,\n",
      "         4.2700e+02, 6.4000e+02],\n",
      "        [4.5832e+05, 0.0000e+00, 1.8789e-01, 8.9953e+01, 7.3359e-02, 1.6310e+01,\n",
      "         4.2700e+02, 6.4000e+02],\n",
      "        [4.5832e+05, 0.0000e+00, 5.5102e-01, 9.6880e+01, 1.0755e-01, 8.8433e+00,\n",
      "         4.2700e+02, 6.4000e+02],\n",
      "        [4.5832e+05, 0.0000e+00, 3.9006e-01, 9.0170e+01, 6.5469e-03, 2.0633e+00,\n",
      "         4.2700e+02, 6.4000e+02],\n",
      "        [4.5832e+05, 0.0000e+00, 2.5331e-01, 5.8470e+01, 1.7734e-02, 7.9833e+00,\n",
      "         4.2700e+02, 6.4000e+02],\n",
      "        [4.5832e+05, 0.0000e+00, 3.7823e-01, 9.4473e+01, 2.8594e-03, 1.4433e+00,\n",
      "         4.2700e+02, 6.4000e+02],\n",
      "        [4.5832e+05, 0.0000e+00, 4.5047e-01, 8.2807e+01, 6.2500e-03, 3.5067e+00,\n",
      "         4.2700e+02, 6.4000e+02],\n",
      "        [4.5832e+05, 0.0000e+00, 6.7908e-01, 9.4553e+01, 1.6672e-02, 4.0767e+00,\n",
      "         4.2700e+02, 6.4000e+02],\n",
      "        [4.5832e+05, 0.0000e+00, 2.3348e-01, 9.5463e+01, 6.7859e-02, 2.6470e+01,\n",
      "         4.2700e+02, 6.4000e+02],\n",
      "        [4.5832e+05, 0.0000e+00, 3.4494e-01, 9.5630e+01, 1.4188e-02, 8.0733e+00,\n",
      "         4.2700e+02, 6.4000e+02],\n",
      "        [4.5832e+05, 0.0000e+00, 3.4606e-01, 9.9237e+01, 2.2875e-02, 1.3770e+01,\n",
      "         4.2700e+02, 6.4000e+02],\n",
      "        [4.5832e+05, 0.0000e+00, 3.9003e-01, 9.0697e+01, 7.0625e-03, 3.0867e+00,\n",
      "         4.2700e+02, 6.4000e+02],\n",
      "        [4.5832e+05, 0.0000e+00, 5.5725e-01, 5.6467e+01, 2.0828e-02, 8.2500e+00,\n",
      "         4.2700e+02, 6.4000e+02],\n",
      "        [4.5832e+05, 0.0000e+00, 2.5469e-01, 9.2333e+01, 3.0625e-01, 2.5667e+01,\n",
      "         4.2700e+02, 6.4000e+02]], dtype=torch.float64), tensor([[5.6906e+05, 0.0000e+00, 3.6130e-01, 3.9433e+01, 2.5578e-01, 5.5810e+01,\n",
      "         4.8000e+02, 6.4000e+02],\n",
      "        [5.6906e+05, 0.0000e+00, 6.0516e-01, 1.6056e+02, 3.9484e-01, 2.4033e+01,\n",
      "         4.8000e+02, 6.4000e+02],\n",
      "        [5.6906e+05, 0.0000e+00, 7.2073e-01, 1.1841e+02, 6.5062e-02, 6.7967e+00,\n",
      "         4.8000e+02, 6.4000e+02],\n",
      "        [5.6906e+05, 0.0000e+00, 3.7934e-01, 1.1584e+02, 3.0375e-01, 2.3980e+01,\n",
      "         4.8000e+02, 6.4000e+02],\n",
      "        [5.6906e+05, 0.0000e+00, 3.0305e-01, 8.6240e+01, 2.0859e-02, 7.4133e+00,\n",
      "         4.8000e+02, 6.4000e+02],\n",
      "        [5.6906e+05, 0.0000e+00, 3.1406e-01, 8.8210e+01, 2.3250e-02, 9.5533e+00,\n",
      "         4.8000e+02, 6.4000e+02],\n",
      "        [5.6906e+05, 0.0000e+00, 6.2105e-01, 9.0347e+01, 7.0813e-02, 5.7833e+00,\n",
      "         4.8000e+02, 6.4000e+02],\n",
      "        [5.6906e+05, 0.0000e+00, 3.2859e-01, 8.8070e+01, 2.3297e-02, 9.4633e+00,\n",
      "         4.8000e+02, 6.4000e+02],\n",
      "        [5.6906e+05, 0.0000e+00, 3.4359e-01, 8.5373e+01, 1.7406e-02, 1.1410e+01,\n",
      "         4.8000e+02, 6.4000e+02],\n",
      "        [5.6906e+05, 0.0000e+00, 3.5512e-01, 8.6257e+01, 1.3141e-02, 3.6733e+00,\n",
      "         4.8000e+02, 6.4000e+02],\n",
      "        [5.6906e+05, 0.0000e+00, 2.8273e-01, 8.6137e+01, 2.8484e-02, 1.1333e+01,\n",
      "         4.8000e+02, 6.4000e+02],\n",
      "        [5.6906e+05, 0.0000e+00, 6.1995e-01, 7.7330e+01, 3.3641e-02, 9.0133e+00,\n",
      "         4.8000e+02, 6.4000e+02],\n",
      "        [5.6906e+05, 0.0000e+00, 3.3025e-01, 8.5920e+01, 1.2500e-02, 5.3667e+00,\n",
      "         4.8000e+02, 6.4000e+02],\n",
      "        [5.6906e+05, 0.0000e+00, 3.5409e-01, 8.4040e+01, 2.2766e-02, 1.2790e+01,\n",
      "         4.8000e+02, 6.4000e+02]], dtype=torch.float64))\n"
     ]
    }
   ],
   "source": [
    "getOneIter(val_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Attack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# attacker = FGSM(model=model, epsilon=0.1)\n",
    "attacker = PGD(model=model, epsilon=0.1)\n",
    "# attacker = CW(model=model, epsilon=0.1, lr=0.02, epoch=5, target=52) # 52 is banana"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/5000 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([500., 375.], device='cuda:0', dtype=torch.float64)\n",
      "tensor([[0.0000e+00, 0.0000e+00, 6.9056e-01, 1.6107e+01, 1.8344e-01, 3.2215e+01],\n",
      "        [0.0000e+00, 0.0000e+00, 6.5644e-01, 1.1814e+01, 8.6660e-02, 2.3628e+01],\n",
      "        [0.0000e+00, 0.0000e+00, 6.3692e-01, 8.8175e+00, 4.2060e-02, 1.7635e+01],\n",
      "        [0.0000e+00, 0.0000e+00, 5.7344e-01, 5.0833e+00, 8.0920e-02, 1.0167e+01],\n",
      "        [0.0000e+00, 0.0000e+00, 6.8230e-01, 3.7391e+01, 1.9170e-01, 7.4782e+01],\n",
      "        [0.0000e+00, 0.0000e+00, 6.4846e-01, 9.1293e+01, 1.1512e-01, 1.6840e+01],\n",
      "        [0.0000e+00, 0.0000e+00, 1.8280e-01, 9.1740e+01, 1.0306e-01, 1.2737e+01],\n",
      "        [0.0000e+00, 0.0000e+00, 2.4891e-01, 9.0087e+01, 4.9782e-01, 7.6580e+01],\n",
      "        [0.0000e+00, 0.0000e+00, 1.3182e-01, 3.5658e+00, 2.6364e-01, 7.1317e+00],\n",
      "        [0.0000e+00, 0.0000e+00, 2.5454e-01, 2.4664e+01, 4.7538e-01, 4.9328e+01],\n",
      "        [0.0000e+00, 0.0000e+00, 6.0022e-01, 2.0142e+00, 4.7560e-02, 4.0283e+00],\n",
      "        [0.0000e+00, 0.0000e+00, 1.2920e-01, 1.8613e+01, 2.5714e-01, 3.5500e+01],\n",
      "        [0.0000e+00, 0.0000e+00, 7.2834e-01, 5.4350e+00, 1.4566e-01, 1.0870e+01]],\n",
      "       device='cuda:0', dtype=torch.float64)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/5000 [00:00<?, ?it/s]\n"
     ]
    }
   ],
   "source": [
    "for i, (images, targets) in enumerate(tqdm(val_loader)):\n",
    "    if targets[0].numel() != 0:\n",
    "        with torch.no_grad():\n",
    "            #* modify inputs to be in proper shape\n",
    "            images = torch.stack(images) # images.shape is [n, 3, 416, 416] (even if n=1)\n",
    "            images = images.to(device)\n",
    "            for i, boxes in enumerate(targets): # targets is nx6, (image,class,x,y,w,h)\n",
    "                if boxes.ndim == 2: boxes[:, 0] = i # change out image_id to id in batch to conform to compute_loss. this is normally done in ListDataset -> collate_fn\n",
    "            targets = torch.cat(targets, 0).to(device) # from tuples to one tensor\n",
    "            originalImageSize = targets[0, 6:] # original image shape, assume one image per batch\n",
    "            targets = targets[:, :6]\n",
    "            \n",
    "            # print(\"#################################\")\n",
    "            #* loss\n",
    "            model.train()\n",
    "            outputs = model(images)\n",
    "            loss, loss_components = compute_loss(outputs, targets, model)\n",
    "            # print(\"before: \", loss)\n",
    "            # print(torch.cuda.memory_allocated())\n",
    "            images_adv = attacker.forward(images, targets) # targets are what to avoid\n",
    "            # print(torch.cuda.memory_allocated())\n",
    "            outputs = model(images_adv)\n",
    "            loss, loss_components = compute_loss(outputs, targets, model)\n",
    "            # print(\"after: \", loss)\n",
    "            \n",
    "            #* plot\n",
    "            model.eval()\n",
    "            outputs = model(images[0].unsqueeze(0))\n",
    "            boxes = non_max_suppression(outputs, conf_thres=0.3, iou_thres=0.5)[0].numpy()\n",
    "            boxes = nms2yolo(boxes, images)\n",
    "            saveImageWithBoxes(images[0], boxes, class_names, \"./output/attack_before.jpg\")\n",
    "            outputs = model(images_adv[0].unsqueeze(0))\n",
    "            boxes = non_max_suppression(outputs, conf_thres=0.3, iou_thres=0.5)[0].numpy()\n",
    "            boxes = nms2yolo(boxes, images_adv)\n",
    "            saveImageWithBoxes(images_adv[0], boxes, class_names, \"./output/attack_after.jpg\")\n",
    "            \n",
    "            # del images, images_adv, targets, outputs, boxes, loss, loss_components\n",
    "            # print(torch.cuda.memory_allocated())\n",
    "    else: continue # pics without targets\n",
    "    \n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Eval for COCOeval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/5000 [00:00<?, ?it/s]\n"
     ]
    }
   ],
   "source": [
    "predictions = []\n",
    "for i, data in enumerate(tqdm(cocoeval_loader)): # * ASSUME ONE IMAGE PER BATCH FOR NOW\n",
    "    with torch.no_grad():\n",
    "        images, targets = data\n",
    "        # print(targets[0])\n",
    "        if len(targets[0]) > 0:\n",
    "            model.eval()\n",
    "            \n",
    "            # * getting necessary values\n",
    "            # for target in targets[0]: print(f\"id {target['image_id']}, class {target['category_id']}, bbox {target['bbox']}\")\n",
    "            image_id = targets[0][0][\"image_id\"]\n",
    "            category_id = targets[0][0][\"category_id\"]\n",
    "            img = images[0].unsqueeze(0).to(device)\n",
    "            img_copy = img.clone().detach()\n",
    "            # print(img.shape) # torch.Size([1, 3, 375, 500])\n",
    "            \n",
    "            # * turn target into 2d numpy matrix for loss calc\n",
    "            # targetsMatrix = np.zeros((len(targets[0]), 6))\n",
    "            # for i, d in enumerate(targets[0]):\n",
    "            #     targetsMatrix[i, 0] = d['image_id']\n",
    "            #     targetsMatrix[i, 1] = d['category_id']\n",
    "            #     targetsMatrix[i, 2:6] = d['bbox']\n",
    "            # np.set_printoptions(suppress=True)\n",
    "            # print(targetsMatrix)\n",
    "\n",
    "            # saveImage(img) # sanity check\n",
    "            \n",
    "            # * put images into model and get bboxes\n",
    "            if modelv == 2:\n",
    "                resized_tensor_image = torch.nn.functional.interpolate(img, size=(416, 416), mode='bilinear', align_corners=False)\n",
    "                boxes = filtered_boxes(model, resized_tensor_image, conf_thresh=0.5, nms_thresh=0.9, device=device) # * default 0.66, 0.55, higher conf -> more strict, higher nms -> more iou needed -> more strict\n",
    "                boxes = np.delete(boxes, 5, axis=1) # x_center, y_center, w, h, bbox_conf, cls_conf, cls -> delete cls_conf\n",
    "            elif modelv == 3:\n",
    "                img = img.cpu().squeeze().permute(1, 2, 0).numpy()\n",
    "                img = transforms.Compose([DEFAULT_TRANSFORMS, ResizeEval(img_size)])((img, np.zeros((1, 5))))[0].unsqueeze(0).to(device) # the np.zeros bit is for label\n",
    "                boxes = model(img)\n",
    "                # print(boxes.shape, img_size)\n",
    "                boxes = non_max_suppression(boxes, conf_thres=0.3, iou_thres=0.5)[0].numpy()\n",
    "                # print(boxes)\n",
    "                boxes = rescale_boxes(boxes, img_size, img_copy.shape[2:]) # rescale back to original proportions?\n",
    "                boxes = nms2yolo(boxes, img_copy)\n",
    "            else: \n",
    "                print(\"invalid model num!\")\n",
    "                \n",
    "            # saveImageWithBoxes(img_copy, boxes, class_names, \"./output/path_to_save_image.jpg\") # for sanity check\n",
    "            predictions += yolo2json(boxes, img_copy, image_id)\n",
    "            \n",
    "        else: continue # pics without labels\n",
    "    break\n",
    "\n",
    "with open(f'./data/results/v{modelv}predictions.json', 'w') as f:\n",
    "    json.dump(predictions, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "put original image size in dataloader targets somehow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compare gt and prediction\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=0.16s)\n",
      "creating index...\n",
      "index created!\n",
      "Loading and preparing results...\n",
      "DONE (t=0.28s)\n",
      "creating index...\n",
      "index created!\n",
      "Running per image evaluation...\n",
      "Evaluate annotation type *bbox*\n",
      "DONE (t=10.97s).\n",
      "Accumulating evaluation results...\n",
      "DONE (t=1.66s).\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.328\n",
      " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.560\n",
      " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.346\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.141\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.360\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.508\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.266\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.372\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.376\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.164\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.407\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.580\n"
     ]
    }
   ],
   "source": [
    "from pycocotools.coco import COCO\n",
    "from pycocotools.cocoeval import COCOeval\n",
    "        \n",
    "coco_gld = COCO(annFile_val) # coco\n",
    "if modelv == 2:\n",
    "    coco_rst = coco_gld.loadRes('./data/results/v2predictions.json')\n",
    "elif modelv == 3:\n",
    "    coco_rst = coco_gld.loadRes('./data/results/v3predictions.json')\n",
    "cocoEval = COCOeval(coco_gld, coco_rst, iouType='bbox')\n",
    "cocoEval.evaluate()\n",
    "cocoEval.accumulate()\n",
    "cocoEval.summarize()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Untitled0.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
