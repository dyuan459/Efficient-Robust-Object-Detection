{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://github.com/miladlink/TinyYoloV2\n",
    "\n",
    "https://github.com/eriklindernoren/PyTorch-YOLOv3\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "WVDh0TibRADG"
   },
   "outputs": [],
   "source": [
    "# import os\n",
    "import time\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import json\n",
    "import cv2\n",
    "from tqdm import tqdm\n",
    "# import skimage.io as io\n",
    "# import matplotlib.pyplot as plt\n",
    "from pycocotools.coco import COCO\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.datasets.coco import CocoDetection\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from utils.YOLOv2 import *\n",
    "from models.YOLOv3 import load_model\n",
    "from attacks.FGSM import FGSM\n",
    "from attacks.PGD import PGD\n",
    "from attacks.CW import CW\n",
    "from attacks.noise import Noise\n",
    "from detect import detect_image\n",
    "from utils.loss import compute_loss\n",
    "from utils.utils import load_classes, rescale_boxes, non_max_suppression, print_environment_info\n",
    "from utils.augmentations import TRANSFORM_TRAIN, TRANSFORM_VAL\n",
    "from utils.transforms import DEFAULT_TRANSFORMS, Resize, ResizeEval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "jlQVknSfeKt4"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelv = 3\n",
    "img_size=416\n",
    "\n",
    "if modelv == 2:\n",
    "    model = load_model_v2(weights = './weights/yolov2-tiny-voc.weights').to(device)\n",
    "    class_names = ['aeroplane', 'bicycle', 'bird', 'boat', 'bottle', 'bus', 'car', 'cat', 'chair', 'cow', 'diningtable', 'dog', 'horse', 'motorbike', 'person', 'pottedplant', 'sheep', 'sofa', 'train', 'TVmonitor'] \n",
    "    root_train = \"./data/VOC2007/JPEGImages\"\n",
    "    annFile_train = \"./data/VOC2007/annotations/train.json\"\n",
    "    root_val = \"./data/VOC2007/JPEGImages\"\n",
    "    annFile_val = \"./data/VOC2007/annotations/val.json\"\n",
    "    \n",
    "elif modelv == 3:\n",
    "    model = load_model(\"./config/yolov3.cfg\", \"./weights/yolov3.weights\")\n",
    "    class_names = ['person', 'bicycle', 'car', 'motorbike', 'aeroplane', 'bus', 'train', 'truck', 'boat', 'traffic light', 'fire hydrant', 'stop sign', 'parking meter', 'bench', 'bird', 'cat', 'dog', 'horse', 'sheep', 'cow', 'elephant', 'bear', 'zebra', 'giraffe', 'backpack', 'umbrella', 'handbag', 'tie', 'suitcase', 'frisbee', 'skis', 'snowboard', 'sports ball', 'kite', 'baseball bat', 'baseball glove', 'skateboard', 'surfboard', 'tennis racket', 'bottle', 'wine glass', 'cup', 'fork', 'knife', 'spoon', 'bowl', 'banana', 'apple', 'sandwich', 'orange', 'broccoli', 'carrot', 'hot dog', 'pizza', 'donut', 'cake', 'chair', 'sofa', 'pottedplant', 'bed', 'diningtable', 'toilet', 'tvmonitor', 'laptop', 'mouse', 'remote', 'keyboard', 'cell phone', 'microwave', 'oven', 'toaster', 'sink', 'refrigerator', 'book', 'clock', 'vase', 'scissors', 'teddy bear', 'hair drier', 'toothbrush']\n",
    "    id_list = np.array([1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 27, 28, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 67, 70, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 84, 85, 86, 87, 88, 89, 90])\n",
    "    root_train = \"./data/COCO2017/train2017\"\n",
    "    annFile_train = \"./data/COCO2017/annotations/instances_train2017_modified.json\"\n",
    "    root_val = \"./data/COCO2017/val2017\"\n",
    "    annFile_val = \"./data/COCO2017/annotations/instances_val2017_modified.json\"\n",
    "    \n",
    "else:\n",
    "    print(\"invalid model number!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Helper functions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def xyxy2xywh(x):\n",
    "    # Convert nx4 boxes from [x1, y1, x2, y2] to [x, y, w, h] where xy1=top-left, xy2=bottom-right\n",
    "    y = x.clone() if isinstance(x, torch.Tensor) else np.copy(x)\n",
    "    y[..., 0] = (x[..., 0] + x[..., 2]) / 2  # x center\n",
    "    y[..., 1] = (x[..., 1] + x[..., 3]) / 2  # y center\n",
    "    y[..., 2] = x[..., 2] - x[..., 0]  # width\n",
    "    y[..., 3] = x[..., 3] - x[..., 1]  # height\n",
    "    return y\n",
    "\n",
    "def xywh2xyxy(x):\n",
    "    # Convert nx4 boxes from [x, y, w, h] to [x1, y1, x2, y2] where xy1=top-left, xy2=bottom-right\n",
    "    y = x.clone() if isinstance(x, torch.Tensor) else np.copy(x)\n",
    "    y[..., 0] = x[..., 0] - x[..., 2] / 2  # top left x\n",
    "    y[..., 1] = x[..., 1] - x[..., 3] / 2  # top left y\n",
    "    y[..., 2] = x[..., 0] + x[..., 2] / 2  # bottom right x\n",
    "    y[..., 3] = x[..., 1] + x[..., 3] / 2  # bottom right y\n",
    "    return y\n",
    "\n",
    "def yolo2json(boxes, img_copy, image_id):\n",
    "    # * put into coco format of x_min,y_min, width, height, bbox_conf, cls\n",
    "    # yolo format is x_center, y_center, w, h, bbox_conf, cls_conf, cls\n",
    "    predictions = []\n",
    "    for box in boxes:\n",
    "        x_center, y_center, w, h, conf, cls = box\n",
    "        x_min = max(0, (x_center - w / 2) * img_copy.shape[3])\n",
    "        y_min = max(0, (y_center - h / 2) * img_copy.shape[2])\n",
    "        width = min(img_copy.shape[3], w * img_copy.shape[3])\n",
    "        height = min(img_copy.shape[2], h * img_copy.shape[2])\n",
    "        # print(x_min,y_min, width, height, bbox_conf, cls)\n",
    "        predictions.append({\n",
    "            'image_id': image_id,\n",
    "            'category_id': int(id_list[int(cls)]) if modelv == 3 else int(cls),\n",
    "            'bbox': [int(x_min), int(y_min), int(width), int(height)],\n",
    "            'score': round(float(conf),2)\n",
    "        })\n",
    "    return predictions\n",
    "\n",
    "def nms2yolo(boxes, img_copy):\n",
    "    boxes = xyxy2xywh(boxes) # convert from coco to yolo: nms returns nx6 (x1, y1, x2, y2, conf, cls), change to center coordinates [x_center, y_center, width, height]\n",
    "    boxes[:,0] = boxes[:,0]/img_copy.shape[3]\n",
    "    boxes[:,1] = boxes[:,1]/img_copy.shape[2]\n",
    "    boxes[:,2] = boxes[:,2]/img_copy.shape[3]\n",
    "    boxes[:,3] = boxes[:,3]/img_copy.shape[2]\n",
    "    return boxes\n",
    "\n",
    "def saveImageWithBoxes(images, boxes, class_names, fileName):  \n",
    "    to_pil = transforms.ToPILImage() \n",
    "    pil_image = to_pil(images.squeeze())\n",
    "    pred_img = plot_boxes(pil_image, boxes, None, class_names)\n",
    "    pred_img.save(fileName)\n",
    "    \n",
    "def saveImage(img):\n",
    "    # * just for sanity check, output image. put the dim 3 at the back\n",
    "    imageN = img.clone().detach()\n",
    "    imageN = imageN.cpu().squeeze().permute(1, 2, 0).numpy() \n",
    "    imageN = cv2.cvtColor(imageN, cv2.COLOR_RGB2BGR)\n",
    "    # print(imageN.shape)\n",
    "    cv2.imwrite(\"data/results/mygraph.jpg\", imageN*255) \n",
    "    \n",
    "def getOneIter(dataloader):\n",
    "    images, annotations = next(iter(dataloader))\n",
    "    np.set_printoptions(linewidth=500)\n",
    "    np.set_printoptions(suppress=True)\n",
    "    print(\"dataloader out\")\n",
    "    print(annotations[0].numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# COCO loader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "create dataloader (make different train and val later)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=0.26s)\n",
      "creating index...\n",
      "index created!\n",
      "loading annotations into memory...\n",
      "Done (t=0.24s)\n",
      "creating index...\n",
      "index created!\n"
     ]
    }
   ],
   "source": [
    "# coco_dataset_train = CocoDetection(root=root_train, annFile=annFile_train, transform=TRANSFORM_TRAIN_IMG, target_transform=TRANSFORM_TRAIN_TARGET)\n",
    "coco_dataset_val = CocoDetection(root=root_val, annFile=annFile_val, transforms=TRANSFORM_VAL)\n",
    "coco_dataset_eval = CocoDetection(root=root_val, annFile=annFile_val, transform=transforms.Compose([transforms.ToTensor(),]))\n",
    "\n",
    "def collate_fn(batch):\n",
    "    return tuple(zip(*batch))\n",
    "\n",
    "# Create a DataLoader for your COCO dataset\n",
    "train_loader = DataLoader(coco_dataset_val, batch_size=4, shuffle=True, collate_fn=collate_fn) # multiple images per batch\n",
    "val_loader = DataLoader(coco_dataset_val, batch_size=1, shuffle=True, collate_fn=collate_fn) # one per batch\n",
    "cocoeval_loader = DataLoader(coco_dataset_eval, batch_size=1, shuffle=True, collate_fn=collate_fn) # original images without transformatios\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataloader out\n",
      "[[216277.             77.              0.61456249      0.23970313      0.21331248      0.2691875     480.            640.        ]\n",
      " [216277.             53.              0.16764454      0.32440624      0.33528907      0.38295311    480.            640.        ]\n",
      " [216277.              1.              0.71003127      0.12803125      0.28996878      0.25345312    480.            640.        ]]\n"
     ]
    }
   ],
   "source": [
    "getOneIter(val_loader) # print targets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Attack Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "attackImage = 0 # variable for saving attack image, run this first, change pruing ratio (attack), don't run this and only run below cells"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# attacker = FGSM(model=model, epsilon=0.05)\n",
    "# attacker = PGD(model=model, epsilon=0.05, epoch=5, lr=0.02)\n",
    "attacker = CW(model=model, epsilon=0.05, lr=0.02, epoch=5, target=52) # 52 is banana\n",
    "# attacker = Noise(model=model, epsilon=0.1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 37%|███▋      | 1851/5000 [00:13<00:53, 58.69it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.0000, 18.0000,  0.1122,  0.3557,  0.2243,  0.3988],\n",
      "        [ 0.0000, 65.0000,  0.2500,  0.5644,  0.5000,  0.2684],\n",
      "        [ 0.0000, 31.0000,  0.3936,  0.4044,  0.3492,  0.2696]],\n",
      "       device='cuda:0', dtype=torch.float64)\n",
      "[[ 62.345375   158.69815    137.37741    302.8995       0.9839087   16.        ]\n",
      " [153.98038    161.80542    286.40512    294.6033       0.5507886   24.        ]\n",
      " [ 26.055977   110.44272     49.973213   188.62592      0.46745738  25.        ]]\n",
      "[[ 0.24005142  0.55480486  0.18036547  0.34663787  0.9839087  16.        ]\n",
      " [ 0.5293095   0.5485682   0.3183287   0.31922567  0.5507886  24.        ]\n",
      " [ 0.09138124  0.3594575   0.05749336  0.18794037  0.46745738 25.        ]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 45%|████▌     | 2264/5000 [00:16<00:20, 135.52it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 10\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;66;03m# mode = \"json\"\u001b[39;00m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;66;03m# image_ids= [71711,19221,22192] # output images that i want\u001b[39;00m\n\u001b[0;32m      8\u001b[0m image_ids\u001b[38;5;241m=\u001b[39m [\u001b[38;5;241m22192\u001b[39m]\n\u001b[1;32m---> 10\u001b[0m \u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mimages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtargets\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtqdm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[0;32m     11\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtargets\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnumel\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m!=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m:\u001b[49m\n\u001b[0;32m     12\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mwith\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mno_grad\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[0;32m     13\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;66;43;03m#* modify inputs to be in proper shape\u001b[39;49;00m\n",
      "File \u001b[1;32md:\\Western\\project\\TinyYoloV2\\env\\Lib\\site-packages\\tqdm\\std.py:1181\u001b[0m, in \u001b[0;36mtqdm.__iter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1178\u001b[0m time \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_time\n\u001b[0;32m   1180\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1181\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43miterable\u001b[49m\u001b[43m:\u001b[49m\n\u001b[0;32m   1182\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01myield\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\n\u001b[0;32m   1183\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Update and possibly print the progressbar.\u001b[39;49;00m\n\u001b[0;32m   1184\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Note: does not call self.update(1) for speed optimisation.\u001b[39;49;00m\n",
      "File \u001b[1;32md:\\Western\\project\\TinyYoloV2\\env\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:631\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    628\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    629\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    630\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 631\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    632\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    633\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    634\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    635\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[1;32md:\\Western\\project\\TinyYoloV2\\env\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:675\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    673\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    674\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> 675\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    676\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[0;32m    677\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[1;32md:\\Western\\project\\TinyYoloV2\\env\\Lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:51\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     49\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[0;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 51\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43midx\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mpossibly_batched_index\u001b[49m\u001b[43m]\u001b[49m\n\u001b[0;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[1;32md:\\Western\\project\\TinyYoloV2\\env\\Lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:51\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     49\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[0;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 51\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[1;32md:\\Western\\project\\TinyYoloV2\\env\\Lib\\site-packages\\torchvision\\datasets\\coco.py:52\u001b[0m, in \u001b[0;36mCocoDetection.__getitem__\u001b[1;34m(self, index)\u001b[0m\n\u001b[0;32m     49\u001b[0m target \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_load_target(\u001b[38;5;28mid\u001b[39m)\n\u001b[0;32m     51\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransforms \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m---> 52\u001b[0m     image, target \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransforms\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     54\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m image, target\n",
      "File \u001b[1;32md:\\Western\\project\\TinyYoloV2\\utils\\augmentations.py:43\u001b[0m, in \u001b[0;36mMyCompose.__call__\u001b[1;34m(self, img, tar)\u001b[0m\n\u001b[0;32m     41\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, img, tar):\n\u001b[0;32m     42\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransforms:\n\u001b[1;32m---> 43\u001b[0m         img, tar \u001b[38;5;241m=\u001b[39m \u001b[43mt\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtar\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     44\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m img, tar\n",
      "File \u001b[1;32md:\\Western\\project\\TinyYoloV2\\utils\\transforms.py:44\u001b[0m, in \u001b[0;36mImgAug.__call__\u001b[1;34m(self, img, boxes)\u001b[0m\n\u001b[0;32m     39\u001b[0m bounding_boxes \u001b[38;5;241m=\u001b[39m BoundingBoxesOnImage(\n\u001b[0;32m     40\u001b[0m     [BoundingBox(\u001b[38;5;241m*\u001b[39mbox[\u001b[38;5;241m2\u001b[39m:\u001b[38;5;241m6\u001b[39m], label\u001b[38;5;241m=\u001b[39mbox[\u001b[38;5;241m0\u001b[39m:\u001b[38;5;241m1\u001b[39m]) \u001b[38;5;28;01mfor\u001b[39;00m box \u001b[38;5;129;01min\u001b[39;00m boxes],\n\u001b[0;32m     41\u001b[0m     shape\u001b[38;5;241m=\u001b[39mimg\u001b[38;5;241m.\u001b[39mshape)\n\u001b[0;32m     43\u001b[0m \u001b[38;5;66;03m# Apply augmentations\u001b[39;00m\n\u001b[1;32m---> 44\u001b[0m img, bounding_boxes \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maugmentations\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     45\u001b[0m \u001b[43m    \u001b[49m\u001b[43mimage\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mimg\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     46\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbounding_boxes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbounding_boxes\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     48\u001b[0m \u001b[38;5;66;03m# Clip out of image boxes\u001b[39;00m\n\u001b[0;32m     49\u001b[0m bounding_boxes \u001b[38;5;241m=\u001b[39m bounding_boxes\u001b[38;5;241m.\u001b[39mclip_out_of_image()\n",
      "File \u001b[1;32md:\\Western\\project\\TinyYoloV2\\env\\Lib\\site-packages\\imgaug\\augmenters\\meta.py:2008\u001b[0m, in \u001b[0;36mAugmenter.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   2006\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m   2007\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Alias for :func:`~imgaug.augmenters.meta.Augmenter.augment`.\"\"\"\u001b[39;00m\n\u001b[1;32m-> 2008\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maugment\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\Western\\project\\TinyYoloV2\\env\\Lib\\site-packages\\imgaug\\augmenters\\meta.py:1979\u001b[0m, in \u001b[0;36mAugmenter.augment\u001b[1;34m(self, return_batch, hooks, **kwargs)\u001b[0m\n\u001b[0;32m   1968\u001b[0m \u001b[38;5;66;03m# augment batch\u001b[39;00m\n\u001b[0;32m   1969\u001b[0m batch \u001b[38;5;241m=\u001b[39m UnnormalizedBatch(\n\u001b[0;32m   1970\u001b[0m     images\u001b[38;5;241m=\u001b[39mimages,\n\u001b[0;32m   1971\u001b[0m     heatmaps\u001b[38;5;241m=\u001b[39mkwargs\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mheatmaps\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1976\u001b[0m     line_strings\u001b[38;5;241m=\u001b[39mkwargs\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mline_strings\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m   1977\u001b[0m )\n\u001b[1;32m-> 1979\u001b[0m batch_aug \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maugment_batch_\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhooks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhooks\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1981\u001b[0m \u001b[38;5;66;03m# return either batch or tuple of augmentables, depending on what\u001b[39;00m\n\u001b[0;32m   1982\u001b[0m \u001b[38;5;66;03m# was requested by user\u001b[39;00m\n\u001b[0;32m   1983\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m return_batch:\n",
      "File \u001b[1;32md:\\Western\\project\\TinyYoloV2\\env\\Lib\\site-packages\\imgaug\\augmenters\\meta.py:641\u001b[0m, in \u001b[0;36mAugmenter.augment_batch_\u001b[1;34m(self, batch, parents, hooks)\u001b[0m\n\u001b[0;32m    639\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m _maybe_deterministic_ctx(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    640\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m batch_inaug\u001b[38;5;241m.\u001b[39mempty:\n\u001b[1;32m--> 641\u001b[0m         batch_inaug \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_augment_batch_\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    642\u001b[0m \u001b[43m            \u001b[49m\u001b[43mbatch_inaug\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    643\u001b[0m \u001b[43m            \u001b[49m\u001b[43mrandom_state\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrandom_state\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    644\u001b[0m \u001b[43m            \u001b[49m\u001b[43mparents\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparents\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mparents\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    645\u001b[0m \u001b[43m            \u001b[49m\u001b[43mhooks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhooks\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    647\u001b[0m \u001b[38;5;66;03m# revert augmentables being set to None for non-activated augmenters\u001b[39;00m\n\u001b[0;32m    648\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m column \u001b[38;5;129;01min\u001b[39;00m set_to_none:\n",
      "File \u001b[1;32md:\\Western\\project\\TinyYoloV2\\env\\Lib\\site-packages\\imgaug\\augmenters\\meta.py:3124\u001b[0m, in \u001b[0;36mSequential._augment_batch_\u001b[1;34m(self, batch, random_state, parents, hooks)\u001b[0m\n\u001b[0;32m   3121\u001b[0m         order \u001b[38;5;241m=\u001b[39m sm\u001b[38;5;241m.\u001b[39mxrange(\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m))\n\u001b[0;32m   3123\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m index \u001b[38;5;129;01min\u001b[39;00m order:\n\u001b[1;32m-> 3124\u001b[0m         batch \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maugment_batch_\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   3125\u001b[0m \u001b[43m            \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3126\u001b[0m \u001b[43m            \u001b[49m\u001b[43mparents\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparents\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3127\u001b[0m \u001b[43m            \u001b[49m\u001b[43mhooks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhooks\u001b[49m\n\u001b[0;32m   3128\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3129\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m batch\n",
      "File \u001b[1;32md:\\Western\\project\\TinyYoloV2\\env\\Lib\\site-packages\\imgaug\\augmenters\\meta.py:641\u001b[0m, in \u001b[0;36mAugmenter.augment_batch_\u001b[1;34m(self, batch, parents, hooks)\u001b[0m\n\u001b[0;32m    639\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m _maybe_deterministic_ctx(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    640\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m batch_inaug\u001b[38;5;241m.\u001b[39mempty:\n\u001b[1;32m--> 641\u001b[0m         batch_inaug \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_augment_batch_\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    642\u001b[0m \u001b[43m            \u001b[49m\u001b[43mbatch_inaug\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    643\u001b[0m \u001b[43m            \u001b[49m\u001b[43mrandom_state\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrandom_state\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    644\u001b[0m \u001b[43m            \u001b[49m\u001b[43mparents\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparents\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mparents\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    645\u001b[0m \u001b[43m            \u001b[49m\u001b[43mhooks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhooks\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    647\u001b[0m \u001b[38;5;66;03m# revert augmentables being set to None for non-activated augmenters\u001b[39;00m\n\u001b[0;32m    648\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m column \u001b[38;5;129;01min\u001b[39;00m set_to_none:\n",
      "File \u001b[1;32md:\\Western\\project\\TinyYoloV2\\env\\Lib\\site-packages\\imgaug\\augmenters\\size.py:2767\u001b[0m, in \u001b[0;36mPadToFixedSize._augment_batch_\u001b[1;34m(self, batch, random_state, parents, hooks)\u001b[0m\n\u001b[0;32m   2764\u001b[0m samples \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_draw_samples(batch, random_state)\n\u001b[0;32m   2766\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m batch\u001b[38;5;241m.\u001b[39mimages \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 2767\u001b[0m     batch\u001b[38;5;241m.\u001b[39mimages \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_augment_images_by_samples\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimages\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2768\u001b[0m \u001b[43m                                                   \u001b[49m\u001b[43msamples\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2770\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m batch\u001b[38;5;241m.\u001b[39mheatmaps \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   2771\u001b[0m     batch\u001b[38;5;241m.\u001b[39mheatmaps \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_augment_maps_by_samples(\n\u001b[0;32m   2772\u001b[0m         batch\u001b[38;5;241m.\u001b[39mheatmaps, samples, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pad_mode_heatmaps,\n\u001b[0;32m   2773\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pad_cval_heatmaps)\n",
      "File \u001b[1;32md:\\Western\\project\\TinyYoloV2\\env\\Lib\\site-packages\\imgaug\\augmenters\\size.py:2803\u001b[0m, in \u001b[0;36mPadToFixedSize._augment_images_by_samples\u001b[1;34m(self, images, samples)\u001b[0m\n\u001b[0;32m   2798\u001b[0m     height_image, width_image \u001b[38;5;241m=\u001b[39m image\u001b[38;5;241m.\u001b[39mshape[:\u001b[38;5;241m2\u001b[39m]\n\u001b[0;32m   2799\u001b[0m     paddings \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_calculate_paddings(height_image, width_image,\n\u001b[0;32m   2800\u001b[0m                                         height_min, width_min,\n\u001b[0;32m   2801\u001b[0m                                         pad_xs[i], pad_ys[i])\n\u001b[1;32m-> 2803\u001b[0m     image \u001b[38;5;241m=\u001b[39m \u001b[43m_crop_and_pad_arr\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   2804\u001b[0m \u001b[43m        \u001b[49m\u001b[43mimage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpaddings\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpad_modes\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpad_cvals\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2805\u001b[0m \u001b[43m        \u001b[49m\u001b[43mkeep_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m   2807\u001b[0m     result\u001b[38;5;241m.\u001b[39mappend(image)\n\u001b[0;32m   2809\u001b[0m \u001b[38;5;66;03m# TODO result is always a list. Should this be converted to an array\u001b[39;00m\n\u001b[0;32m   2810\u001b[0m \u001b[38;5;66;03m#      if possible (not guaranteed that all images have same size,\u001b[39;00m\n\u001b[0;32m   2811\u001b[0m \u001b[38;5;66;03m#      some might have been larger than desired height/width)\u001b[39;00m\n",
      "File \u001b[1;32md:\\Western\\project\\TinyYoloV2\\env\\Lib\\site-packages\\imgaug\\augmenters\\size.py:81\u001b[0m, in \u001b[0;36m_crop_and_pad_arr\u001b[1;34m(arr, croppings, paddings, pad_mode, pad_cval, keep_size)\u001b[0m\n\u001b[0;32m     77\u001b[0m height, width \u001b[38;5;241m=\u001b[39m arr\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m:\u001b[38;5;241m2\u001b[39m]\n\u001b[0;32m     79\u001b[0m image_cr \u001b[38;5;241m=\u001b[39m _crop_arr_(arr, \u001b[38;5;241m*\u001b[39mcroppings)\n\u001b[1;32m---> 81\u001b[0m image_cr_pa \u001b[38;5;241m=\u001b[39m \u001b[43mpad\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     82\u001b[0m \u001b[43m    \u001b[49m\u001b[43mimage_cr\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     83\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpaddings\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mright\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpaddings\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     84\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbottom\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpaddings\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mleft\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpaddings\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     85\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpad_mode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpad_cval\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     87\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m keep_size:\n\u001b[0;32m     88\u001b[0m     image_cr_pa \u001b[38;5;241m=\u001b[39m ia\u001b[38;5;241m.\u001b[39mimresize_single_image(image_cr_pa, (height, width))\n",
      "File \u001b[1;32md:\\Western\\project\\TinyYoloV2\\env\\Lib\\site-packages\\imgaug\\augmenters\\size.py:501\u001b[0m, in \u001b[0;36mpad\u001b[1;34m(arr, top, right, bottom, left, mode, cval)\u001b[0m\n\u001b[0;32m    497\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m arr\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m3\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_multi_cval:\n\u001b[0;32m    498\u001b[0m     cval \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mtuple\u001b[39m([cval] \u001b[38;5;241m*\u001b[39m arr\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m2\u001b[39m])\n\u001b[0;32m    500\u001b[0m arr_pad \u001b[38;5;241m=\u001b[39m cv2\u001b[38;5;241m.\u001b[39mcopyMakeBorder(\n\u001b[1;32m--> 501\u001b[0m     \u001b[43m_normalize_cv2_input_arr_\u001b[49m\u001b[43m(\u001b[49m\u001b[43marr\u001b[49m\u001b[43m)\u001b[49m,\n\u001b[0;32m    502\u001b[0m     top\u001b[38;5;241m=\u001b[39mtop, bottom\u001b[38;5;241m=\u001b[39mbottom, left\u001b[38;5;241m=\u001b[39mleft, right\u001b[38;5;241m=\u001b[39mright,\n\u001b[0;32m    503\u001b[0m     borderType\u001b[38;5;241m=\u001b[39mmapping_mode_np_to_cv2[mode], value\u001b[38;5;241m=\u001b[39mcval)\n\u001b[0;32m    504\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m arr\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m3\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m arr_pad\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m2\u001b[39m:\n\u001b[0;32m    505\u001b[0m     arr_pad \u001b[38;5;241m=\u001b[39m arr_pad[\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m, np\u001b[38;5;241m.\u001b[39mnewaxis]\n",
      "File \u001b[1;32md:\\Western\\project\\TinyYoloV2\\env\\Lib\\site-packages\\imgaug\\imgaug.py:2150\u001b[0m, in \u001b[0;36m_normalize_cv2_input_arr_\u001b[1;34m(arr)\u001b[0m\n\u001b[0;32m   2148\u001b[0m flags \u001b[38;5;241m=\u001b[39m arr\u001b[38;5;241m.\u001b[39mflags\n\u001b[0;32m   2149\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m flags[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOWNDATA\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n\u001b[1;32m-> 2150\u001b[0m     arr \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m(\u001b[49m\u001b[43marr\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2151\u001b[0m     flags \u001b[38;5;241m=\u001b[39m arr\u001b[38;5;241m.\u001b[39mflags\n\u001b[0;32m   2152\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m flags[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mC_CONTIGUOUS\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n",
      "File \u001b[1;32md:\\Western\\project\\TinyYoloV2\\env\\Lib\\site-packages\\numpy\\lib\\function_base.py:962\u001b[0m, in \u001b[0;36mcopy\u001b[1;34m(a, order, subok)\u001b[0m\n\u001b[0;32m    873\u001b[0m \u001b[38;5;129m@array_function_dispatch\u001b[39m(_copy_dispatcher)\n\u001b[0;32m    874\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcopy\u001b[39m(a, order\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mK\u001b[39m\u001b[38;5;124m'\u001b[39m, subok\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[0;32m    875\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    876\u001b[0m \u001b[38;5;124;03m    Return an array copy of the given object.\u001b[39;00m\n\u001b[0;32m    877\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    960\u001b[0m \n\u001b[0;32m    961\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 962\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m array(a, order\u001b[38;5;241m=\u001b[39morder, subok\u001b[38;5;241m=\u001b[39msubok, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "predictionsBefore = []\n",
    "predictionsAfter = []\n",
    "lossesBefore = []\n",
    "lossesAfter = []\n",
    "mode = \"image\" # need different modes if i want to save image or output prediction json\n",
    "# mode = \"json\"\n",
    "# image_ids= [71711,19221,22192] # output images that i want, 19221 is broccoli, 22191 is dog, 71711 is plane\n",
    "image_ids= [19221]\n",
    "\n",
    "for i, (images, targets) in enumerate(tqdm(val_loader)):\n",
    "    if targets[0].numel() != 0:\n",
    "        with torch.no_grad():\n",
    "            #* modify inputs to be in proper shape\n",
    "            images = torch.stack(images) # images.shape is [n, 3, 416, 416] (even if n=1)\n",
    "            images = images.to(device)\n",
    "            image_id = int(targets[0][0,0].cpu().numpy()) # assume 1 image\n",
    "            if image_id not in image_ids: continue # for when we want outputs of specific images\n",
    "            for i, boxes in enumerate(targets): # targets is nx6, (image,class,x,y,w,h)\n",
    "                if boxes.ndim == 2: boxes[:, 0] = i # change out image_id to id in batch to conform to compute_loss. this is normally done in ListDataset -> collate_fn. the id now starts at 0 for each image\n",
    "            targets = torch.cat(targets, 0).to(device) # from tuples to one tensor\n",
    "            originalImageSize = targets[0, 6:].cpu().numpy() # original image shape, assume one image per batch\n",
    "            targets = targets[:, :6]\n",
    "            \n",
    "            #* loss\n",
    "            model.train()\n",
    "            # start = time.time()\n",
    "            outputsBefore = model(images)\n",
    "            # end = time.time()\n",
    "            # print(end - start)\n",
    "            lossBefore, loss_components = compute_loss(outputsBefore, targets, model)\n",
    "            lossesBefore.append(lossBefore.cpu().numpy())\n",
    "            \n",
    "            images_adv = attacker.forward(images, targets) # get adversarial image\n",
    "            \n",
    "            outputsAfter = model(images_adv)\n",
    "            lossAfter, loss_components = compute_loss(outputsAfter, targets, model)\n",
    "            lossesAfter.append(lossAfter.cpu().numpy())\n",
    "            \n",
    "            #* plot\n",
    "            model.eval()\n",
    "            \n",
    "            # ground truth\n",
    "            print(targets) #(image,class,x,y,w,h), the class id starts from 1\n",
    "            # nms is (x1, y1, x2, y2, conf, cls), the class id starts from 0\n",
    "            # yolo is (x_center, y_center, width, height, conf. cls)\n",
    "            \n",
    "            # before attack\n",
    "            outputsBefore = model(images[0].unsqueeze(0))\n",
    "            boxesBefore = non_max_suppression(outputsBefore, conf_thres=0.3, iou_thres=0.5)[0].numpy()\n",
    "            if mode == \"json\":\n",
    "                boxesBefore = rescale_boxes(boxesBefore, img_size, originalImageSize)\n",
    "            boxesBefore = nms2yolo(boxesBefore, images)\n",
    "            if mode == \"image\":\n",
    "                saveImageWithBoxes(images[0], boxesBefore, class_names, f\"./data/results/images/attack_before_{image_id}.jpg\")\n",
    "            if mode == \"json\":\n",
    "                predictionsBefore += yolo2json(boxesBefore, images[0].unsqueeze(0), image_id)\n",
    "                \n",
    "            # after attack\n",
    "            outputsAfter = model(images_adv[0].unsqueeze(0))\n",
    "            boxesAfter = non_max_suppression(outputsAfter, conf_thres=0.3, iou_thres=0.5)[0].numpy()\n",
    "            \n",
    "            \n",
    "            if mode == \"json\":\n",
    "                boxesAfter = rescale_boxes(boxesAfter, img_size, originalImageSize)\n",
    "            print(boxesAfter)\n",
    "            boxesAfter = nms2yolo(boxesAfter, images_adv)\n",
    "            print(boxesAfter)\n",
    "            if mode == \"image\":\n",
    "                # attackImage = images_adv[0] # for saving the same attack image for different pruning ratios\n",
    "                saveImageWithBoxes(images_adv[0], boxesAfter, class_names, f\"./data/results/images/attack_after_{image_id}.jpg\")\n",
    "                # saveImageWithBoxes(attackImage, attackPredictions, class_names, f\"./data/results/images/pruning/attack_after_{image_id}_90new.jpg\") # plot different pruning ratios with same attack image\n",
    "            if mode == \"json\":\n",
    "                predictionsAfter += yolo2json(boxesAfter, images_adv[0].unsqueeze(0), image_id)\n",
    "            attackPredictions = boxesAfter\n",
    "            # time.sleep(0.1) # for using noise attack\n",
    "            \n",
    "    else: continue # pics without targets\n",
    "    # break\n",
    "\n",
    "\n",
    "with open(f'./data/results/predictionsBefore.json', 'w') as f:\n",
    "    json.dump(predictionsBefore, f)\n",
    "with open(f'./data/results/predictionsAfter.json', 'w') as f:\n",
    "    json.dump(predictionsAfter, f)\n",
    "np.savetxt(\"./data/results/lossesBefore.csv\", lossesBefore, delimiter=\",\")\n",
    "np.savetxt(\"./data/results/lossesAfter.csv\", lossesAfter, delimiter=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg loss before attack: 0.041589983851871355\n",
      "Avg loss after attack: 0.037373899639318676\n"
     ]
    }
   ],
   "source": [
    "data = np.loadtxt('./data/results/lossesBefore.csv', delimiter=',')\n",
    "average = np.mean(data)\n",
    "print(\"Avg loss before attack:\", average)\n",
    "data = np.loadtxt('./data/results/lossesAfter.csv', delimiter=',')\n",
    "average = np.mean(data)\n",
    "print(\"Avg loss after attack:\", average)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adversarial training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "losses = []\n",
    "epochs = 10\n",
    "checkpoint_interval = 1\n",
    "params = [p for p in model.parameters() if p.requires_grad]\n",
    "optimizer = optim.Adam(\n",
    "            params,\n",
    "            lr=model.hyperparams['learning_rate'],\n",
    "            weight_decay=model.hyperparams['decay'],\n",
    "        )\n",
    "\n",
    "for epoch in range(1, epochs+1):\n",
    "    for i, (images, targets) in enumerate(tqdm(train_loader)):\n",
    "        model.train()\n",
    "        lossesEpoch = []\n",
    "        if targets[0].numel() != 0:\n",
    "            #* modify inputs to be in proper shape\n",
    "            images = torch.stack(images) # images.shape is [n, 3, 416, 416] (even if n=1)\n",
    "            images = images.to(device)\n",
    "            for i, boxes in enumerate(targets): # targets is nx6, (image,class,x,y,w,h)\n",
    "                if boxes.ndim == 2: boxes[:, 0] = i # change out image_id to id in batch to conform to compute_loss. this is normally done in ListDataset -> collate_fn\n",
    "            targets = torch.cat(targets, 0).to(device) # from tuples to one tensor\n",
    "            targets = targets[:, :6]\n",
    "            # if image_id not in image_ids: continue # for when we want outputs of specific images\n",
    "            \n",
    "            images_adv = attacker.forward(images, targets) # get adversarial image\n",
    "            outputsBefore = model(images)\n",
    "            lossBefore, loss_components = compute_loss(outputsBefore, targets, model)\n",
    "            outputsAfter = model(images_adv)\n",
    "            lossAfter, loss_components = compute_loss(outputsAfter, targets, model)\n",
    "            loss = lossBefore + lossAfter\n",
    "            lossesEpoch.append(loss.cpu().numpy())\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            # Reset gradients\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            time.sleep(0.1) # for using noise attack\n",
    "        else: continue # pics without targets\n",
    "    losses = np.average(lossesEpoch)\n",
    "            \n",
    "    if epoch % checkpoint_interval == 0:\n",
    "        checkpoint_path = f\"./data/results/checkpoints/yolov3_ckpt_{epoch}.pth\"\n",
    "        print(f\"---- Saving checkpoint to: '{checkpoint_path}' ----\")\n",
    "        torch.save(model.state_dict(), checkpoint_path)\n",
    "            \n",
    "    # break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get mAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=0.15s)\n",
      "creating index...\n",
      "index created!\n",
      "Loading and preparing results...\n",
      "DONE (t=0.00s)\n",
      "creating index...\n",
      "index created!\n",
      "Loading and preparing results...\n",
      "DONE (t=0.08s)\n",
      "creating index...\n",
      "index created!\n",
      "Running per image evaluation...\n",
      "Evaluate annotation type *bbox*\n",
      "DONE (t=9.27s).\n",
      "Accumulating evaluation results...\n",
      "DONE (t=1.45s).\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.266\n",
      " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.504\n",
      " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.259\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.116\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.315\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.408\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.224\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.317\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.321\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.137\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.362\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.486\n"
     ]
    }
   ],
   "source": [
    "from pycocotools.coco import COCO\n",
    "from pycocotools.cocoeval import COCOeval\n",
    "        \n",
    "coco_gld = COCO(annFile_val) # coco\n",
    "if modelv == 2:\n",
    "    coco_rst = coco_gld.loadRes('./data/results/v2predictions.json')\n",
    "elif modelv == 3:\n",
    "    coco_rst = coco_gld.loadRes('./data/results/v3predictions.json')\n",
    "    \n",
    "coco_rst = coco_gld.loadRes('./data/results/predictionsAfter.json')\n",
    "cocoEval = COCOeval(coco_gld, coco_rst, iouType='bbox')\n",
    "cocoEval.evaluate()\n",
    "cocoEval.accumulate()\n",
    "cocoEval.summarize()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Untitled0.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
