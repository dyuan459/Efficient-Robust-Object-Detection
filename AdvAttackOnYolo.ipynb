{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://github.com/miladlink/TinyYoloV2\n",
    "\n",
    "https://github.com/eriklindernoren/PyTorch-YOLOv3\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "WVDh0TibRADG"
   },
   "outputs": [],
   "source": [
    "\n",
    "# import os\n",
    "import time\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import json\n",
    "import cv2\n",
    "from tqdm import tqdm\n",
    "# import skimage.io as io\n",
    "# import matplotlib.pyplot as plt\n",
    "from pycocotools.coco import COCO\n",
    "import torch\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.datasets.coco import CocoDetection\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from utils.YOLOv2 import *\n",
    "from models.YOLOv3 import load_model\n",
    "from attacks.FGSM import FGSM\n",
    "from attacks.PGD import PGD\n",
    "from attacks.CW import CW\n",
    "from attacks.noise import Noise\n",
    "from detect import detect_image\n",
    "from utils.loss import compute_loss\n",
    "from utils.utils import load_classes, rescale_boxes, non_max_suppression, print_environment_info\n",
    "from utils.augmentations import TRANSFORM_TRAIN, TRANSFORM_VAL\n",
    "from utils.transforms import DEFAULT_TRANSFORMS, Resize, ResizeEval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "jlQVknSfeKt4"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelv = 3\n",
    "img_size=416\n",
    "\n",
    "if modelv == 2:\n",
    "    model = load_model_v2(weights = './weights/yolov2-tiny-voc.weights').to(device)\n",
    "    class_names = ['aeroplane', 'bicycle', 'bird', 'boat', 'bottle', 'bus', 'car', 'cat', 'chair', 'cow', 'diningtable', 'dog', 'horse', 'motorbike', 'person', 'pottedplant', 'sheep', 'sofa', 'train', 'TVmonitor'] \n",
    "    root_train = \"./data/VOC2007/JPEGImages\"\n",
    "    annFile_train = \"./data/VOC2007/annotations/train.json\"\n",
    "    root_val = \"./data/VOC2007/JPEGImages\"\n",
    "    annFile_val = \"./data/VOC2007/annotations/val.json\"\n",
    "    \n",
    "elif modelv == 3:\n",
    "    model = load_model(\"./config/yolov3.cfg\", \"./weights/yolov3.weights\")\n",
    "    class_names = ['person', 'bicycle', 'car', 'motorbike', 'aeroplane', 'bus', 'train', 'truck', 'boat', 'traffic light', 'fire hydrant', 'stop sign', 'parking meter', 'bench', 'bird', 'cat', 'dog', 'horse', 'sheep', 'cow', 'elephant', 'bear', 'zebra', 'giraffe', 'backpack', 'umbrella', 'handbag', 'tie', 'suitcase', 'frisbee', 'skis', 'snowboard', 'sports ball', 'kite', 'baseball bat', 'baseball glove', 'skateboard', 'surfboard', 'tennis racket', 'bottle', 'wine glass', 'cup', 'fork', 'knife', 'spoon', 'bowl', 'banana', 'apple', 'sandwich', 'orange', 'broccoli', 'carrot', 'hot dog', 'pizza', 'donut', 'cake', 'chair', 'sofa', 'pottedplant', 'bed', 'diningtable', 'toilet', 'tvmonitor', 'laptop', 'mouse', 'remote', 'keyboard', 'cell phone', 'microwave', 'oven', 'toaster', 'sink', 'refrigerator', 'book', 'clock', 'vase', 'scissors', 'teddy bear', 'hair drier', 'toothbrush']\n",
    "    id_list = np.array([1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 27, 28, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 67, 70, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 84, 85, 86, 87, 88, 89, 90])\n",
    "    root_train = \"./data/COCO2017/train2017\"\n",
    "    annFile_train = \"./data/COCO2017/annotations/instances_train2017_modified.json\"\n",
    "    root_val = \"./data/COCO2017/val2017\"\n",
    "    annFile_val = \"./data/COCO2017/annotations/instances_val2017_modified.json\"\n",
    "    \n",
    "else:\n",
    "    print(\"invalid model number!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# helper functions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def xyxy2xywh(x):\n",
    "    # Convert nx4 boxes from [x1, y1, x2, y2] to [x, y, w, h] where xy1=top-left, xy2=bottom-right\n",
    "    y = x.clone() if isinstance(x, torch.Tensor) else np.copy(x)\n",
    "    y[..., 0] = (x[..., 0] + x[..., 2]) / 2  # x center\n",
    "    y[..., 1] = (x[..., 1] + x[..., 3]) / 2  # y center\n",
    "    y[..., 2] = x[..., 2] - x[..., 0]  # width\n",
    "    y[..., 3] = x[..., 3] - x[..., 1]  # height\n",
    "    return y\n",
    "\n",
    "def xywh2xyxy(x):\n",
    "    # Convert nx4 boxes from [x, y, w, h] to [x1, y1, x2, y2] where xy1=top-left, xy2=bottom-right\n",
    "    y = x.clone() if isinstance(x, torch.Tensor) else np.copy(x)\n",
    "    y[..., 0] = x[..., 0] - x[..., 2] / 2  # top left x\n",
    "    y[..., 1] = x[..., 1] - x[..., 3] / 2  # top left y\n",
    "    y[..., 2] = x[..., 0] + x[..., 2] / 2  # bottom right x\n",
    "    y[..., 3] = x[..., 1] + x[..., 3] / 2  # bottom right y\n",
    "    return y\n",
    "\n",
    "def yolo2json(boxes, img_copy, image_id):\n",
    "    # * put into coco format of x_min,y_min, width, height, bbox_conf, cls\n",
    "    # yolo format is x_center, y_center, w, h, bbox_conf, cls_conf, cls\n",
    "    predictions = []\n",
    "    for box in boxes:\n",
    "        x_center, y_center, w, h, conf, cls = box\n",
    "        x_min = max(0, (x_center - w / 2) * img_copy.shape[3])\n",
    "        y_min = max(0, (y_center - h / 2) * img_copy.shape[2])\n",
    "        width = min(img_copy.shape[3], w * img_copy.shape[3])\n",
    "        height = min(img_copy.shape[2], h * img_copy.shape[2])\n",
    "        # print(x_min,y_min, width, height, bbox_conf, cls)\n",
    "        predictions.append({\n",
    "            'image_id': image_id,\n",
    "            'category_id': int(id_list[int(cls)]) if modelv == 3 else int(cls),\n",
    "            'bbox': [int(x_min), int(y_min), int(width), int(height)],\n",
    "            'score': round(float(conf),2)\n",
    "        })\n",
    "    return predictions\n",
    "\n",
    "def nms2yolo(boxes, img_copy):\n",
    "    boxes = xyxy2xywh(boxes) # convert from coco to yolo: nms returns nx6 (x1, y1, x2, y2, conf, cls), change to center\n",
    "    boxes[:,0] = boxes[:,0]/img_copy.shape[3]\n",
    "    boxes[:,1] = boxes[:,1]/img_copy.shape[2]\n",
    "    boxes[:,2] = boxes[:,2]/img_copy.shape[3]\n",
    "    boxes[:,3] = boxes[:,3]/img_copy.shape[2]\n",
    "    return boxes\n",
    "\n",
    "def saveImageWithBoxes(images, boxes, class_names, fileName):  \n",
    "    to_pil = transforms.ToPILImage() \n",
    "    pil_image = to_pil(images.squeeze())\n",
    "    pred_img = plot_boxes(pil_image, boxes, None, class_names)\n",
    "    pred_img.save(fileName)\n",
    "    \n",
    "def saveImage(img):\n",
    "    # * just for sanity check, output image. put the dim 3 at the back\n",
    "    imageN = img.clone().detach()\n",
    "    imageN = imageN.cpu().squeeze().permute(1, 2, 0).numpy() \n",
    "    imageN = cv2.cvtColor(imageN, cv2.COLOR_RGB2BGR)\n",
    "    # print(imageN.shape)\n",
    "    cv2.imwrite(\"data/results/mygraph.jpg\", imageN*255) \n",
    "    \n",
    "def getOneIter(dataloader):\n",
    "    images, annotations = next(iter(dataloader))\n",
    "    np.set_printoptions(linewidth=500)\n",
    "    np.set_printoptions(suppress=True)\n",
    "    print(\"dataloader out\")\n",
    "    print(annotations[0].numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# COCO loader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "create dataloader (make different train and val later)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=0.24s)\n",
      "creating index...\n",
      "index created!\n",
      "loading annotations into memory...\n",
      "Done (t=0.25s)\n",
      "creating index...\n",
      "index created!\n"
     ]
    }
   ],
   "source": [
    "# coco_dataset_train = CocoDetection(root=root_train, annFile=annFile_train, transform=TRANSFORM_TRAIN_IMG, target_transform=TRANSFORM_TRAIN_TARGET)\n",
    "coco_dataset_val = CocoDetection(root=root_val, annFile=annFile_val, transforms=TRANSFORM_VAL)\n",
    "coco_dataset_eval = CocoDetection(root=root_val, annFile=annFile_val, transform=transforms.Compose([transforms.ToTensor(),]))\n",
    "\n",
    "def collate_fn(batch):\n",
    "    return tuple(zip(*batch))\n",
    "\n",
    "# Create a DataLoader for your COCO dataset\n",
    "train_loader = DataLoader(coco_dataset_val, batch_size=4, shuffle=True, collate_fn=collate_fn) # multiple images per batch\n",
    "val_loader = DataLoader(coco_dataset_val, batch_size=1, shuffle=True, collate_fn=collate_fn) # one per batch\n",
    "cocoeval_loader = DataLoader(coco_dataset_eval, batch_size=1, shuffle=True, collate_fn=collate_fn) # original images without transformatios\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataloader out\n",
      "[[524108.              0.              0.20716407     75.65666771      0.41432815     96.18000158    400.            640.        ]\n",
      " [524108.              0.              0.28054688     59.4766655       0.56109376    112.65666453    400.            640.        ]\n",
      " [524108.              0.              0.24287501     54.93000031      0.44465627     25.11000061    400.            640.        ]\n",
      " [524108.              0.              0.20329688     41.56333335      0.24053125     10.89666653    400.            640.        ]\n",
      " [524108.              0.              0.180875       42.60333316      0.36175001     37.96333186    400.            640.        ]\n",
      " [524108.              0.              0.92287502     65.08000104      0.07712498     56.32333533    400.            640.        ]\n",
      " [524108.              0.              0.86376562     51.15999985      0.13060932     38.65333303    400.            640.        ]\n",
      " [524108.              0.              0.04845703     41.49666723      0.09691406     46.74333445    400.            640.        ]\n",
      " [524108.              0.              0.41064063     40.              0.23607814     17.88999939    400.            640.        ]\n",
      " [524108.              0.              0.77306252     85.22000122      0.2119688      41.89666748    400.            640.        ]\n",
      " [524108.              0.              0.66207812     46.18999974      0.15767188     20.37666607    400.            640.        ]\n",
      " [524108.              0.              0.19010937     46.94333315      0.305125       10.16999976    400.            640.        ]]\n"
     ]
    }
   ],
   "source": [
    "getOneIter(val_loader) # print targets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Attack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# attacker = FGSM(model=model, epsilon=0.05)\n",
    "# attacker = PGD(model=model, epsilon=0.1, epoch=5, lr=0.02)\n",
    "# attacker = CW(model=model, epsilon=0.1, lr=0.02, epoch=5, target=52) # 52 is banana\n",
    "attacker = Noise(model=model, epsilon=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5000/5000 [16:24<00:00,  5.08it/s]\n"
     ]
    }
   ],
   "source": [
    "predictionsBefore = []\n",
    "predictionsAfter = []\n",
    "lossesBefore = []\n",
    "lossesAfter = []\n",
    "# mode = \"image\" # need different modes if i want to save image or output prediction json\n",
    "mode = \"json\"\n",
    "image_ids= [71711,19221,22192]\n",
    "\n",
    "for i, (images, targets) in enumerate(tqdm(val_loader)):\n",
    "    if targets[0].numel() != 0:\n",
    "        with torch.no_grad():\n",
    "            #* modify inputs to be in proper shape\n",
    "            images = torch.stack(images) # images.shape is [n, 3, 416, 416] (even if n=1)\n",
    "            images = images.to(device)\n",
    "            image_id = int(targets[0][0,0].cpu().numpy())\n",
    "            for i, boxes in enumerate(targets): # targets is nx6, (image,class,x,y,w,h)\n",
    "                if boxes.ndim == 2: boxes[:, 0] = i # change out image_id to id in batch to conform to compute_loss. this is normally done in ListDataset -> collate_fn\n",
    "            targets = torch.cat(targets, 0).to(device) # from tuples to one tensor\n",
    "            originalImageSize = targets[0, 6:].cpu().numpy() # original image shape, assume one image per batch\n",
    "            targets = targets[:, :6]\n",
    "            # if image_id not in image_ids: continue # for when we want outputs of specific images\n",
    "            \n",
    "            #* loss\n",
    "            model.train()\n",
    "            \n",
    "            outputsBefore = model(images)\n",
    "            lossBefore, loss_components = compute_loss(outputsBefore, targets, model)\n",
    "            lossesBefore.append(lossBefore.cpu().numpy())\n",
    "            \n",
    "            images_adv = attacker.forward(images, targets) # get adversarial image\n",
    "            \n",
    "            outputsAfter = model(images_adv)\n",
    "            lossAfter, loss_components = compute_loss(outputsAfter, targets, model)\n",
    "            lossesAfter.append(lossAfter.cpu().numpy())\n",
    "            \n",
    "            #* plot\n",
    "            model.eval()\n",
    "            \n",
    "            outputsBefore = model(images[0].unsqueeze(0))\n",
    "            boxesBefore = non_max_suppression(outputsBefore, conf_thres=0.3, iou_thres=0.5)[0].numpy()\n",
    "            if mode == \"json\":\n",
    "                boxesBefore = rescale_boxes(boxesBefore, img_size, originalImageSize)\n",
    "            boxesBefore = nms2yolo(boxesBefore, images)\n",
    "            if mode == \"image\":\n",
    "                saveImageWithBoxes(images[0], boxesBefore, class_names, f\"./data/results/images/attack_before.jpg\")\n",
    "            if mode == \"json\":\n",
    "                predictionsBefore += yolo2json(boxesBefore, images[0].unsqueeze(0), image_id)\n",
    "                \n",
    "            outputsAfter = model(images_adv[0].unsqueeze(0))\n",
    "            boxesAfter = non_max_suppression(outputsAfter, conf_thres=0.3, iou_thres=0.5)[0].numpy()\n",
    "            if mode == \"json\":\n",
    "                boxesAfter = rescale_boxes(boxesAfter, img_size, originalImageSize)\n",
    "            boxesAfter = nms2yolo(boxesAfter, images_adv)\n",
    "            if mode == \"image\":\n",
    "                saveImageWithBoxes(images_adv[0], boxesAfter, class_names, f\"./data/results/images/attack_after.jpg\")\n",
    "            if mode == \"json\":\n",
    "                predictionsAfter += yolo2json(boxesAfter, images_adv[0].unsqueeze(0), image_id)\n",
    "            \n",
    "            time.sleep(0.1) # for using noise attack\n",
    "            \n",
    "    else: continue # pics without targets\n",
    "    # break\n",
    "\n",
    "\n",
    "with open(f'./data/results/predictionsBefore.json', 'w') as f:\n",
    "    json.dump(predictionsBefore, f)\n",
    "with open(f'./data/results/predictionsAfter.json', 'w') as f:\n",
    "    json.dump(predictionsAfter, f)\n",
    "np.savetxt(\"./data/results/lossesBefore.csv\", lossesBefore, delimiter=\",\")\n",
    "np.savetxt(\"./data/results/lossesAfter.csv\", lossesAfter, delimiter=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg loss before attack: 0.041589983851871355\n",
      "Avg loss after attack: 0.037373899639318676\n"
     ]
    }
   ],
   "source": [
    "data = np.loadtxt('./data/results/lossesBefore.csv', delimiter=',')\n",
    "average = np.mean(data)\n",
    "print(\"Avg loss before attack:\", average)\n",
    "data = np.loadtxt('./data/results/lossesAfter.csv', delimiter=',')\n",
    "average = np.mean(data)\n",
    "print(\"Avg loss after attack:\", average)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Eval for COCOeval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/5000 [00:00<?, ?it/s]\n"
     ]
    }
   ],
   "source": [
    "predictions = []\n",
    "for i, data in enumerate(tqdm(cocoeval_loader)): # * ASSUME ONE IMAGE PER BATCH FOR NOW\n",
    "    with torch.no_grad():\n",
    "        images, targets = data\n",
    "        # print(targets[0])\n",
    "        if len(targets[0]) > 0:\n",
    "            model.eval()\n",
    "            \n",
    "            # * getting necessary values\n",
    "            # for target in targets[0]: print(f\"id {target['image_id']}, class {target['category_id']}, bbox {target['bbox']}\")\n",
    "            image_id = targets[0][0][\"image_id\"]\n",
    "            category_id = targets[0][0][\"category_id\"]\n",
    "            img = images[0].unsqueeze(0).to(device)\n",
    "            img_copy = img.clone().detach()\n",
    "            # print(img.shape) # torch.Size([1, 3, 375, 500])\n",
    "            \n",
    "            # * turn target into 2d numpy matrix for loss calc\n",
    "            # targetsMatrix = np.zeros((len(targets[0]), 6))\n",
    "            # for i, d in enumerate(targets[0]):\n",
    "            #     targetsMatrix[i, 0] = d['image_id']\n",
    "            #     targetsMatrix[i, 1] = d['category_id']\n",
    "            #     targetsMatrix[i, 2:6] = d['bbox']\n",
    "            # np.set_printoptions(suppress=True)\n",
    "            # print(targetsMatrix)\n",
    "\n",
    "            # saveImage(img) # sanity check\n",
    "            \n",
    "            # * put images into model and get bboxes\n",
    "            if modelv == 2:\n",
    "                resized_tensor_image = torch.nn.functional.interpolate(img, size=(416, 416), mode='bilinear', align_corners=False)\n",
    "                boxes = filtered_boxes(model, resized_tensor_image, conf_thresh=0.5, nms_thresh=0.9, device=device) # * default 0.66, 0.55, higher conf -> more strict, higher nms -> more iou needed -> more strict\n",
    "                boxes = np.delete(boxes, 5, axis=1) # x_center, y_center, w, h, bbox_conf, cls_conf, cls -> delete cls_conf\n",
    "            elif modelv == 3:\n",
    "                img = img.cpu().squeeze().permute(1, 2, 0).numpy()\n",
    "                img = transforms.Compose([DEFAULT_TRANSFORMS, ResizeEval(img_size)])((img, np.zeros((1, 5))))[0].unsqueeze(0).to(device) # the np.zeros bit is for label\n",
    "                boxes = model(img)\n",
    "                # print(boxes.shape, img_size)\n",
    "                boxes = non_max_suppression(boxes, conf_thres=0.3, iou_thres=0.5)[0].numpy()\n",
    "                # print(boxes)\n",
    "                boxes = rescale_boxes(boxes, img_size, img_copy.shape[2:]) # rescale back to original proportions?\n",
    "                boxes = nms2yolo(boxes, img_copy)\n",
    "            else: \n",
    "                print(\"invalid model num!\")\n",
    "                \n",
    "            # saveImageWithBoxes(img_copy, boxes, class_names, \"./data/results/path_to_save_image.jpg\") # for sanity check\n",
    "            predictions += yolo2json(boxes, img_copy, image_id)\n",
    "            \n",
    "        else: continue # pics without labels\n",
    "    break\n",
    "\n",
    "with open(f'./data/results/v{modelv}predictions.json', 'w') as f:\n",
    "    json.dump(predictions, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compare gt and prediction\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=0.15s)\n",
      "creating index...\n",
      "index created!\n",
      "Loading and preparing results...\n",
      "DONE (t=0.00s)\n",
      "creating index...\n",
      "index created!\n",
      "Loading and preparing results...\n",
      "DONE (t=0.08s)\n",
      "creating index...\n",
      "index created!\n",
      "Running per image evaluation...\n",
      "Evaluate annotation type *bbox*\n",
      "DONE (t=9.27s).\n",
      "Accumulating evaluation results...\n",
      "DONE (t=1.45s).\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.266\n",
      " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.504\n",
      " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.259\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.116\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.315\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.408\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.224\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.317\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.321\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.137\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.362\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.486\n"
     ]
    }
   ],
   "source": [
    "from pycocotools.coco import COCO\n",
    "from pycocotools.cocoeval import COCOeval\n",
    "        \n",
    "coco_gld = COCO(annFile_val) # coco\n",
    "if modelv == 2:\n",
    "    coco_rst = coco_gld.loadRes('./data/results/v2predictions.json')\n",
    "elif modelv == 3:\n",
    "    coco_rst = coco_gld.loadRes('./data/results/v3predictions.json')\n",
    "    \n",
    "coco_rst = coco_gld.loadRes('./data/results/predictionsAfter.json')\n",
    "cocoEval = COCOeval(coco_gld, coco_rst, iouType='bbox')\n",
    "cocoEval.evaluate()\n",
    "cocoEval.accumulate()\n",
    "cocoEval.summarize()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Untitled0.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
